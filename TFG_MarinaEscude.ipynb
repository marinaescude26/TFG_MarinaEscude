{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891725c1",
   "metadata": {},
   "source": [
    "# TREBALL DE FI DE GRAU\n",
    "\n",
    "## ACCIDENTS DE TRÀNSIT: Predicció i factors de risc a través de models estadístics\n",
    "\n",
    "### 1. INTRODUCCIÓ\n",
    "#### 1.1 OBJECTIU\n",
    "#### 1.2 BASE DE DADES\n",
    "### 2. PREPROCESSAMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llibreries necessàries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import imblearn\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.experimental import enable_iterative_imputer, enable_halving_search_cv  \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, HalvingGridSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfcce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importem les dades\n",
    "dades = pd.read_csv('roadaccident.csv')\n",
    "dades.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5cd71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = dades.shape\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ff4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompte NA\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "recompte_NA = dades.isna().sum()\n",
    "recompte_NA = pd.DataFrame({'VARIABLE': recompte_NA.index, 'NA': recompte_NA.values})\n",
    "\n",
    "recompte_NA['% NA'] = (recompte_NA['NA'] / len(dades)) * 100   # percentatge (%)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x = 'VARIABLE', y = '% NA', data = recompte_NA, hue = 'VARIABLE', color = '#228B22', legend=False)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('PERCENTATGE DE VALORS PERDUTS PER VARIABLE', fontsize = 15)\n",
    "plt.ylabel('% NA', fontsize = 11)\n",
    "\n",
    "for index, value in enumerate (recompte_NA['% NA']):\n",
    "    plt.text(index, value, f'{round(value,3)}%', ha = 'center', va = 'bottom', \n",
    "             color = 'black', rotation = 0, fontsize = 8, weight = 'bold')\n",
    "plt.savefig('NA.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valors atípics \n",
    "dades_numeriques = dades[['Number_of_Casualties','Number_of_Vehicles','Speed_limit']]\n",
    "columnes = dades_numeriques.columns\n",
    "n = len(dades)\n",
    "\n",
    "for i in columnes:\n",
    "    Q1 = dades[i].quantile(0.25)\n",
    "    Q3 = dades[i].quantile(0.75)\n",
    "    \n",
    "    Rang_Interquartilic = Q3 - Q1\n",
    "    \n",
    "    L_sup = Q3 + 1.5 * Rang_Interquartilic\n",
    "    L_inf = Q1 - 1.5 * Rang_Interquartilic\n",
    "    \n",
    "    Valors_atipics = dades[(dades[i] < L_inf) | (dades[i] > L_sup)]\n",
    "    Num_outliers = len(Valors_atipics)\n",
    "    Percentatge_outliers = (Num_outliers*100)/n\n",
    "\n",
    "    print('\\n La quantitat de valors atípics de la variable',i ,f'és {Num_outliers}, que representa un {round(Percentatge_outliers,2)} % del total dels casos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd366957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos duplicats\n",
    "dades.duplicated().any()\n",
    "duplicats = dades[dades.duplicated()]\n",
    "num_duplicats = duplicats.shape[0]\n",
    "\n",
    "# Eliminem casos duplicats\n",
    "dades = dades.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82958fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Creem variables noves\n",
    "dades['Accident Date'] = pd.to_datetime(dades['Accident Date'], format='%m/%d/%Y')\n",
    "dades['Any_Accident'] = dades['Accident Date'].dt.year\n",
    "dades['Mes_Accident'] = dades['Accident Date'].dt.month\n",
    "dades['Dia_Accident'] = dades['Accident Date'].dt.day\n",
    "\n",
    "# Eliminem variables redundants\n",
    "dades = dades.drop(columns = ['Accident_Index']) \n",
    "dades.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprovem de quina classe és cada variable\n",
    "dades.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8bc67",
   "metadata": {},
   "source": [
    "Analitzem totes les variables de la base de dades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a916973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accident Date\n",
    "summary_Accident_Date = dades['Accident Date'].describe(datetime_is_numeric=True)\n",
    "\n",
    "# Day_of_Week\n",
    "dades['Day_of_Week'] = dades['Day_of_Week'].astype('category')\n",
    "summary_Day_of_Week = dades['Day_of_Week'].describe()\n",
    "classes_Day_of_Week = dades['Day_of_Week'].cat.categories\n",
    "casos_per_classe_Day_of_Week = dades['Day_of_Week'].value_counts()\n",
    "##print(casos_per_classe_Day_of_Week)\n",
    "\n",
    "# Junction_Control\n",
    "dades['Junction_Control'] = dades['Junction_Control'].astype('category')\n",
    "summary_Junction_Control = dades['Junction_Control'].describe()\n",
    "classes_Junction_Control = dades['Junction_Control'].cat.categories\n",
    "casos_per_classe_Junction_Control = dades['Junction_Control'].value_counts()\n",
    "##print(casos_per_classe_Junction_Control)\n",
    "\n",
    "# Junction_Detail\n",
    "dades['Junction_Detail'] = dades['Junction_Detail'].astype('category')\n",
    "summary_Junction_Detail = dades['Junction_Detail'].describe()\n",
    "classes_Junction_Detail = dades['Junction_Detail'].cat.categories\n",
    "casos_per_classe_Junction_Detail = dades['Junction_Detail'].value_counts()\n",
    "##print(casos_per_classe_Junction_Detail)\n",
    "\n",
    "# Accident_Severity\n",
    "dades['Accident_Severity'] = dades['Accident_Severity'].astype('category')\n",
    "summary_Accident_Severity = dades['Accident_Severity'].describe()\n",
    "classes_Accident_Severity = dades['Accident_Severity'].cat.categories\n",
    "casos_per_classe_Accident_Severity = dades['Accident_Severity'].value_counts()\n",
    "##print(casos_per_classe_Accident_Severity)\n",
    "\n",
    "# Latitude\n",
    "summary_Latitude = dades['Latitude'].describe()\n",
    "\n",
    "# Light_Conditions\n",
    "dades['Light_Conditions'] = dades['Light_Conditions'].astype('category')\n",
    "summary_Light_Conditions = dades['Light_Conditions'].describe()\n",
    "classes_Light_Conditions = dades['Light_Conditions'].cat.categories\n",
    "casos_per_classe_Light_Conditions = dades['Light_Conditions'].value_counts()\n",
    "##print(casos_per_classe_Light_Conditions)\n",
    "\n",
    "# Local_Authority_(District)\n",
    "dades['Local_Authority_(District)'] = dades['Local_Authority_(District)'].astype('category')\n",
    "summary_Local_Authority_District = dades['Local_Authority_(District)'].describe()\n",
    "classes_Local_Authority_District = dades['Local_Authority_(District)'].cat.categories\n",
    "casos_per_classe_Local_Authority_District = dades['Local_Authority_(District)'].value_counts()\n",
    "##print(casos_per_classe_Local_Authority_District)\n",
    "\n",
    "# Carriageway_Hazards\n",
    "dades['Carriageway_Hazards'] = dades['Carriageway_Hazards'].astype('category')\n",
    "summary_Carriageway_Hazards = dades['Carriageway_Hazards'].describe()\n",
    "classes_Carriageway_Hazards = dades['Carriageway_Hazards'].cat.categories\n",
    "casos_per_classe_Carriageway_Hazards = dades['Carriageway_Hazards'].value_counts()\n",
    "##print(casos_per_classe_Carriageway_Hazards)\n",
    "\n",
    "# Longitude\n",
    "summary_Longitude = dades['Longitude'].describe()\n",
    "\n",
    "# Number_of_Casualties\n",
    "summary_Number_of_Casualties = dades['Number_of_Casualties'].describe()\n",
    "taula_freq_Number_of_Casualties = dades['Number_of_Casualties'].value_counts()\n",
    "\n",
    "# Number_of_Vehicles\n",
    "summary_Number_of_Vehicles = dades['Number_of_Vehicles'].describe()\n",
    "taula_freq_Number_of_Vehicles = dades['Number_of_Vehicles'].value_counts()\n",
    "\n",
    "# Police_Force\n",
    "dades['Police_Force'] = dades['Police_Force'].astype('category')\n",
    "summary_Police_Force = dades['Police_Force'].describe()\n",
    "classes_Police_Force = dades['Police_Force'].cat.categories\n",
    "casos_per_classe_Police_Force = dades['Police_Force'].value_counts()\n",
    "##print(casos_per_classe_Police_Force)\n",
    "\n",
    "# Road_Surface_Conditions\n",
    "dades['Road_Surface_Conditions'] = dades['Road_Surface_Conditions'].astype('category')\n",
    "summary_Road_Surface_Conditions = dades['Road_Surface_Conditions'].describe()\n",
    "classes_Road_Surface_Conditions = dades['Road_Surface_Conditions'].cat.categories\n",
    "casos_per_classe_Road_Surface_Conditions = dades['Road_Surface_Conditions'].value_counts()\n",
    "##print(casos_per_classe_Road_Surface_Conditions)\n",
    "\n",
    "# Road_Type\n",
    "dades['Road_Type'] = dades['Road_Type'].astype('category')\n",
    "summary_Road_Type = dades['Road_Type'].describe()\n",
    "classes_Road_Type = dades['Road_Type'].cat.categories\n",
    "casos_per_classe_Road_Type = dades['Road_Type'].value_counts()\n",
    "##print(casos_per_classe_Road_Type)\n",
    "\n",
    "# Speed_limit\n",
    "summary_Speed_limit = dades['Speed_limit'].describe()\n",
    "taula_freq_Speed_limit = dades['Speed_limit'].value_counts()\n",
    "\n",
    "# Time\n",
    "dades['Time'] = dades['Time'].astype('category')\n",
    "summary_Time = dades['Time'].describe()\n",
    "classes_Time = dades['Time'].cat.categories\n",
    "casos_per_classe_Time = dades['Time'].value_counts()\n",
    "##print(casos_per_classe_Time)\n",
    "\n",
    "# Urban_or_Rural_Area\n",
    "dades['Urban_or_Rural_Area'] = dades['Urban_or_Rural_Area'].astype('category')\n",
    "summary_Urban_or_Rural_Area = dades['Urban_or_Rural_Area'].describe()\n",
    "classes_Urban_or_Rural_Area = dades['Urban_or_Rural_Area'].cat.categories\n",
    "casos_per_classe_Urban_or_Rural_Area = dades['Urban_or_Rural_Area'].value_counts()\n",
    "##print(casos_per_classe_Urban_or_Rural_Area)\n",
    "\n",
    "# Weather_Conditions \n",
    "dades['Weather_Conditions'] = dades['Weather_Conditions'].astype('category')\n",
    "summary_Weather_Conditions = dades['Weather_Conditions'].describe()\n",
    "classes_Weather_Conditions = dades['Weather_Conditions'].cat.categories\n",
    "casos_per_classe_Weather_Conditions = dades['Weather_Conditions'].value_counts()\n",
    "##print(casos_per_classe_Weather_Conditions)\n",
    "\n",
    "# Vehicle_Type\n",
    "dades['Vehicle_Type'] = dades['Vehicle_Type'].astype('category')\n",
    "summary_Vehicle_Type = dades['Vehicle_Type'].describe()\n",
    "classes_Vehicle_Type = dades['Vehicle_Type'].cat.categories\n",
    "casos_per_classe_Vehicle_Type = dades['Vehicle_Type'].value_counts()\n",
    "##print(casos_per_classe_Vehicle_Type)\n",
    "\n",
    "# Any_Accident\n",
    "dades['Any_Accident'] = dades['Any_Accident'].astype('category')\n",
    "summary_Any_Accident = dades['Any_Accident'].describe()\n",
    "classes_Any_Accident = dades['Any_Accident'].cat.categories\n",
    "casos_per_classe_Any_Accident = dades['Any_Accident'].value_counts()\n",
    "##print(casos_per_classe_Any_Accident)\n",
    "\n",
    "# Month_Accident\n",
    "summary_Mes_Accident = dades['Mes_Accident'].describe()\n",
    "taula_freq_Mes_Accident = dades['Mes_Accident'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprovem el canvi de tipus object a categòric\n",
    "dades.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206238f6",
   "metadata": {},
   "source": [
    "### 3. ANÀLISI DESCRIPTIVA\n",
    "#### 3.1 ANÀLISI DESCRIPTIVA QUANTITATIVA\n",
    "\n",
    "Gràfics previs a realitzar canvis en certes variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3e7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariant\n",
    "\n",
    "# Number_of_Casualties\n",
    "frequencia_casualties = dades['Number_of_Casualties'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(frequencia_casualties.index, frequencia_casualties.values, color = '#008000')\n",
    "plt.xlim(0, max(frequencia_casualties.index) + 1)\n",
    "plt.xlabel('Number of Casualties')\n",
    "plt.ylabel('Freqüència')\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS PER NOMBRE DE VÍCTIMES', fontsize = 15)\n",
    "for i, v in zip(frequencia_casualties.index, frequencia_casualties.values):\n",
    "    plt.text(i, v + 0.1, str(v), ha='center', va='bottom', color='black', fontsize=6)\n",
    "#plt.savefig('gqt_casualties_1.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Number_of_Vehicles\n",
    "frequencia_vehicles = dades['Number_of_Vehicles'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(frequencia_vehicles.index, frequencia_vehicles.values, color = '#008000', width = 0.8)\n",
    "plt.xlim(0, max(frequencia_vehicles.index) + 1)\n",
    "plt.xlabel('Number of Vehicles')\n",
    "plt.ylabel('Freqüència')\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS PER NOMBRE DE VEHICLES', fontsize = 15)\n",
    "for i, v in zip(frequencia_vehicles.index, frequencia_vehicles.values):\n",
    "    plt.text(i , v + 0.1, str(v), ha='center', va='bottom', color='black', fontsize=6)\n",
    "#plt.savefig('gqt_NumVehicles_1.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Speed_Limit\n",
    "frequencia_velocitat = dades['Speed_limit'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(frequencia_velocitat.index, frequencia_velocitat.values, color = '#008000', width = 4)\n",
    "plt.xlabel('Speed Limit')\n",
    "plt.ylabel('Freqüència')\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS DELS DIFERENTS LÍMITS DE VELOCITAT', fontsize = 15)\n",
    "for i, v in zip(frequencia_velocitat.index, frequencia_velocitat.values):\n",
    "    plt.text(i , v + 0.1, str(v), ha='center', va='bottom', color='black', fontsize=6)\n",
    "#plt.savefig('gqt_SpeedLimit_1.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Mes_Accident\n",
    "x = ['Gener','Febrer','Març','Abril','Maig','Juny','Juliol',' Agost', 'Setembre','Octubre', 'Novembre', 'Desembre']\n",
    "frequencia_mes = dades['Mes_Accident'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(frequencia_mes.index, frequencia_mes.values, color = '#008000', width = 0.9)\n",
    "plt.xlabel('Mes Accident')\n",
    "plt.ylabel('Freqüència')\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS PER MES', fontsize = 15)\n",
    "plt.xticks(ticks = range(1, len(x) + 1), labels = x, rotation = 90, fontsize = 8)\n",
    "#plt.savefig('gq_MesAccidentt.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Dia_Accident\n",
    "x = list(range(1,32))\n",
    "frequencia_dia = dades['Dia_Accident'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(frequencia_dia.index, frequencia_dia.values, color = '#008000', width = 0.9)\n",
    "plt.xlabel('Dia Accident')\n",
    "plt.ylabel('Freqüència')\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS PER DIA', fontsize = 15)\n",
    "plt.xticks(ticks = range(1, len(x) + 1), labels = x, rotation = 90, fontsize = 8)\n",
    "#plt.savefig('gq_DiaAccidentt.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# UNIVARIANT - QUALITATIVA\n",
    "\n",
    "# Vehicle_Type\n",
    "frequencia_vehicletype = dades['Vehicle_Type'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(frequencia_vehicletype.index, frequencia_vehicletype.values, color = '#008000', width = 0.9)\n",
    "plt.xlabel('Vehicle Type')\n",
    "plt.ylabel('Freqüència')\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS DELS DIFERENTS TIPUS DE VEHICLES', fontsize = 15)\n",
    "plt.xticks(rotation=90, fontsize=8)\n",
    "for i, v in zip(frequencia_vehicletype.index, frequencia_vehicletype.values):\n",
    "    plt.text(i , v + 0.1, str(v), ha='center', va='bottom', color='black', fontsize=7)\n",
    "#plt.savefig('gq_TypeVehicle_1.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariant\n",
    "\n",
    "# Gràfic dispersió Speed Limit i Number of Casualties\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(dades['Speed_limit'], dades['Number_of_Casualties'],color = '#008000')\n",
    "plt.yticks(range(0, max(dades['Number_of_Casualties']) + 1, 5))\n",
    "\n",
    "plt.xlabel('Speed Limit')\n",
    "plt.ylabel('Number of Casualties')\n",
    "plt.title('RELACIÓ ENTRE EL LÍMIT DE VELOCITAT I EL NOMBRE DE VÍCTIMES EN ELS ACCIDENTS', fontsize = 15)\n",
    "#plt.savefig('gqtqt_SpeedCasualties_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "correlation1 = np.corrcoef(dades['Speed_limit'], dades['Number_of_Casualties'])[0, 1]\n",
    "\n",
    "# Gràfic dispersió Number of vehicles i Number of Casualties\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(dades['Number_of_Vehicles'], dades['Number_of_Casualties'],color = '#008000')\n",
    "plt.yticks(range(0, max(dades['Number_of_Casualties']) + 1, 5))\n",
    "\n",
    "plt.xlabel('Number of Vehicles')\n",
    "plt.ylabel('Number of Casualties')\n",
    "plt.title('RELACIÓ ENTRE EL NOMBRE DE VEHICLES I EL NOMBRE DE VÍCTIMES EN ELS ACCIDENTS', fontsize = 15)\n",
    "#plt.savefig('gqtqt_NumVehiclesCasualties_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "correlation2 = np.corrcoef(dades['Number_of_Vehicles'], dades['Number_of_Casualties'])[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf3293c",
   "metadata": {},
   "source": [
    "**Apliquem les modificacions necessàries:**\n",
    "\n",
    "- Accident Severity: Passar fetal a fatal (error ortogràfic)\n",
    "- Junction_Control: Passar 'Auto traffic sigl' a 'Auto traffic signal' (error ortogràfic)\n",
    "- Number_of_Casualities: posar >=4 com a nova categoria en comptes de (4,5,6,7,8,9...)\n",
    "- Number_of_Vehicles: posar >=4 com a nova categoria en comptes de (4,5,6,7,8,9...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eefea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accident_Severity - error ortogràfic\n",
    "dades['Accident_Severity'] = dades['Accident_Severity'].replace('Fetal','Fatal') \n",
    "\n",
    "# Junction_Control - error ortogràfic\n",
    "dades['Junction_Control'] = dades['Junction_Control'].replace('Auto traffic sigl',\n",
    "                                                              'Auto traffic signal')\n",
    "\n",
    "# Number_of_Casualties\n",
    "dades['Number_of_Casualties'] = pd.Categorical(dades['Number_of_Casualties'], ordered=True)\n",
    "\n",
    "if '4 o més' not in dades['Number_of_Casualties'].cat.categories:\n",
    "    dades['Number_of_Casualties'] = dades['Number_of_Casualties'].cat.add_categories('4 o més')\n",
    "    \n",
    "dades.loc[dades['Number_of_Casualties'] >= 4, 'Number_of_Casualties'] = '4 o més'\n",
    "\n",
    "categoria_a_eliminar = [42,43,48,40,19,21,22,24,26,27,13,14,15,16,17,18,12,11,10,9,8,7,6,4,5]\n",
    "categories_presents = dades['Number_of_Casualties'].cat.categories\n",
    "categories_a_eliminar2 = [categoria for categoria in categoria_a_eliminar if categoria in categories_presents]\n",
    "\n",
    "if categories_a_eliminar2:\n",
    "    dades['Number_of_Casualties'] = dades['Number_of_Casualties'].cat.remove_categories(categories_a_eliminar2)\n",
    "\n",
    "# Number_of_Vehicles\n",
    "dades['Number_of_Vehicles'] = pd.Categorical(dades['Number_of_Vehicles'], ordered=True)\n",
    "\n",
    "if '4 o més' not in dades['Number_of_Vehicles'].cat.categories:\n",
    "    dades['Number_of_Vehicles'] = dades['Number_of_Vehicles'].cat.add_categories('4 o més')\n",
    "    \n",
    "dades.loc[dades['Number_of_Vehicles'] >=4, 'Number_of_Vehicles'] = \"4 o més\"\n",
    "\n",
    "categoria_a_eliminar2 = [4,5,6,7,8,9,10,11,12,13,14,16,19,32]\n",
    "categories_presents2 = dades['Number_of_Vehicles'].cat.categories\n",
    "categories_a_eliminar22 = [categoria for categoria in categoria_a_eliminar2 if categoria in categories_presents2]\n",
    "\n",
    "if categories_a_eliminar22:\n",
    "    dades['Number_of_Vehicles'] = dades['Number_of_Vehicles'].cat.remove_categories(categories_a_eliminar22)\n",
    "    \n",
    "# Speed_Limit\n",
    "dades['Speed_limit'] = dades['Speed_limit'].astype('category')\n",
    "categories_a_eliminar3 = [10,15]\n",
    "dades['Speed_limit'] = dades['Speed_limit'].cat.remove_categories(categories_a_eliminar3)\n",
    "\n",
    "\n",
    "# Vehicle_Type\n",
    "dades['Vehicle_Type'] = pd.Categorical(dades['Vehicle_Type'], ordered=True)\n",
    "\n",
    "dades.loc[dades['Vehicle_Type'] == \"Pedal cycle\", 'Vehicle_Type'] = \"Other vehicle\"\n",
    "dades.loc[dades['Vehicle_Type'] == \"Ridden horse\", 'Vehicle_Type'] = \"Other vehicle\"\n",
    "\n",
    "categoria_a_eliminar3 = [\"Pedal cycle\", \"Ridden horse\"]\n",
    "categories_presents3 = dades['Vehicle_Type'].cat.categories\n",
    "categories_a_eliminar33 = [categoria for categoria in categoria_a_eliminar3 if categoria in categories_presents3]\n",
    "\n",
    "if categories_a_eliminar33:\n",
    "    dades['Vehicle_Type'] = dades[\"Vehicle_Type\"].cat.remove_categories(categories_a_eliminar33)\n",
    "    \n",
    "# Light_Conditions\n",
    "dades['Light_Conditions'] = pd.Categorical(dades['Light_Conditions'], ordered=True)\n",
    "\n",
    "dades.loc[dades['Light_Conditions'] == \"Darkness - lights unlit\", 'Light_Conditions'] = \"Darkness - no lighting\"\n",
    "\n",
    "categoria_a_eliminar4 = [\"Darkness - lights unlit\"]\n",
    "categories_presents4 = dades['Light_Conditions'].cat.categories\n",
    "categories_a_eliminar44 = [categoria for categoria in categoria_a_eliminar4 if categoria in categories_presents4]\n",
    "\n",
    "if categories_a_eliminar44:\n",
    "    dades['Light_Conditions'] = pd.Categorical(dades['Light_Conditions'].cat.remove_categories(categories_a_eliminar44))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870ea77",
   "metadata": {},
   "source": [
    "Gràfics després d'aplicar modificacions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariant\n",
    "\n",
    "# Number_of_Casualties\n",
    "categories = ['1', '2', '3', '4 o més']\n",
    "frequencia_casualties = dades['Number_of_Casualties'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(categories, frequencia_casualties, color = '#008000')\n",
    "plt.xlabel('Number of Casualties', fontsize = 16)\n",
    "plt.ylabel('Freqüència', fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS PER NOMBRE DE VÍCTIMES', fontsize = 17)\n",
    "for i, v in zip(categories, frequencia_casualties):\n",
    "    plt.text(i, v + 0.1, str(v), ha='center', va='bottom', color='black', fontsize = 14)\n",
    "#plt.savefig('gqt_casualties_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Number_of_Vehicles\n",
    "categories = ['1', '2', '3', '4 o més']\n",
    "frequencia_vehicles = dades['Number_of_Vehicles'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(categories, frequencia_vehicles, color = '#008000', width = 0.8)\n",
    "plt.xlabel('Number of Vehicles', fontsize = 16)\n",
    "plt.ylabel('Freqüència', fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS PER NOMBRE DE VEHÍCLES', fontsize = 17)\n",
    "for i, v in zip(categories, frequencia_vehicles):\n",
    "    plt.text(i , v + 0.1, str(v), ha='center', va='bottom', color='black', fontsize = 14)\n",
    "#plt.savefig('gqt_NumVehicles_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Speed_Limit\n",
    "dades = dades.dropna(subset = ['Speed_limit'])\n",
    "frequencia_velocitat = dades['Speed_limit'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(frequencia_velocitat.index, frequencia_velocitat.values, color = '#008000', width = 4)\n",
    "plt.xlabel('Speed Limit', fontsize = 15)\n",
    "plt.ylabel('Freqüència', fontsize = 15)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS AMB ELS DIFERENTS LÍMITS DE VELOCITAT', fontsize = 15)\n",
    "for i, v in zip(frequencia_velocitat.index, frequencia_velocitat.values):\n",
    "    plt.text(i , v + 0.1, str(v), ha='center', va='bottom', color='black', fontsize = 14)\n",
    "#plt.savefig('gqt_SpeedLimit_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Any_Accident\n",
    "frequencia_any = dades['Any_Accident'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(frequencia_any, labels=frequencia_any.index, autopct='%1.1f%%', \n",
    "        textprops={'fontsize': 14, 'color': 'white'}, startangle=90, \n",
    "        colors=['#008000', '#DAA520'])\n",
    "plt.legend(labels=frequencia_any.index, title='Year', loc='upper right', fontsize = 15, title_fontsize = 17)\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS PER ANY', fontsize = 15)\n",
    "#plt.savefig('gqt_YearAccident_2.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariant (1)\n",
    "\n",
    "## Speed Limit + Number of Casualties \n",
    "\n",
    "### Barres apilades\n",
    "cross_table = pd.crosstab(dades['Speed_limit'],dades['Number_of_Casualties'])\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "cross_table.plot(kind='bar', color = ['#008000', '#DAA520', '#FF0000', '#5F9EA0'], stacked=True, figsize=(9, 7))\n",
    "\n",
    "plt.legend(title='Number of Casualties', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.xlabel(\"Speed Limit\", fontsize = 11)\n",
    "plt.yticks(fontsize = 11)\n",
    "plt.xticks(fontsize = 11)\n",
    "plt.title('PROPORCIÓ DEL NOMBRE DE VÍCTIMES EN ELS DIFERENTS\\nLÍMITS DE VELOCITAT', fontsize = 15)\n",
    "#plt.savefig('gqtqt_SpeedCasualties_1.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "### Diagrama de violí\n",
    "dades_new = dades.copy()\n",
    "\n",
    "speed_limit_order = sorted(dades_new['Speed_limit'].unique())\n",
    "dades_new['Speed_limit_numeric'] = pd.to_numeric(dades_new['Speed_limit'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.violinplot(x=dades_new['Number_of_Casualties'], y=dades_new['Speed_limit_numeric'], \n",
    "               scale=\"width\", palette= ['#008000', '#DAA520', '#FF0000', '#5F9EA0'])\n",
    "plt.yticks(speed_limit_order, fontsize = 15)\n",
    "plt.xticks(ticks=[0, 1, 2, 3], labels=['1', '2', '3', '4 o més'], fontsize=15)\n",
    "plt.xlabel('Number of Casualties', fontsize =  15)\n",
    "plt.ylabel('Speed Limit', fontsize = 15)\n",
    "plt.title('GRÀFIC DE VIOLÍ DEL LÍMIT DE VELOCITAT SEGONS\\nEL NOMBRE DE VÍCTIMES', fontsize = 17)\n",
    "#plt.savefig('gqtqt_SpeedCasualties3.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "### BoxPlot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(x=dades_new['Number_of_Casualties'], y=dades_new['Speed_limit_numeric'], \n",
    "            palette= ['#008000', '#DAA520', '#FF0000', '#5F9EA0'])\n",
    "plt.yticks(speed_limit_order)\n",
    "plt.xlabel('Number of Casualties', fontsize = 16)\n",
    "plt.ylabel('Speed Limit', fontsize = 16)\n",
    "plt.title('GRÀFIC DE CAIXES DEL LÍMIT DE VELOCITAT SEGONS\\nEL NOMBRE DE VÍCTIMES', fontsize = 17)\n",
    "plt.xticks(ticks=[0, 1, 2, 3], labels=['1', '2', '3', '4 o més'], fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#plt.savefig('gqtqt_SpeedCasualties_boxplot.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariant (2)\n",
    "\n",
    "## Road Surface Conditions i Speed limit\n",
    "dades['Speed_limit'] = pd.to_numeric(dades['Speed_limit'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(x='Road_Surface_Conditions', y='Speed_limit', data=dades, color = \"#008000\")\n",
    "plt.xlabel('Road Surface Conditions')\n",
    "plt.ylabel('Speed Limit')\n",
    "plt.xticks(fontsize = 11)\n",
    "plt.yticks(fontsize = 11)\n",
    "plt.title('ANÀLISI DELS LÍMITS DE VELOCITAT EN DIFERENTS CONDICIONS\\nDE LA SUPERFÍCIE DE LA CARRETERA', fontsize = 15)\n",
    "#plt.savefig('gqtql_SurfaceSpeed.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "## Accident_Severity i Speed limit\n",
    "\n",
    "### Boxplot\n",
    "dades['Speed_limit'] = pd.to_numeric(dades['Speed_limit'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(x='Accident_Severity', y='Speed_limit', data=dades, palette=[\"red\",\"orange\",\"green\"])\n",
    "plt.xlabel('Accident_Severity')\n",
    "plt.ylabel('Speed Limit')\n",
    "plt.xticks(fontsize = 11)\n",
    "plt.yticks(fontsize = 11)\n",
    "plt.title('ANÀLISI DELS LÍMITS DE VELOCITAT EN DIFERENTS\\nGRAUS DE GRAVETAT DELS ACCIDENTS', fontsize = 15)\n",
    "#plt.savefig('gqtql_RespSpeed_1.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "### Diagrama de violí\n",
    "dades['Speed_limit'] = pd.to_numeric(dades['Speed_limit'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.violinplot(x='Accident_Severity', y='Speed_limit', data=dades, palette=[\"red\",\"orange\",\"green\"])\n",
    "plt.xlabel('Accident Severity', fontsize = 16)\n",
    "plt.ylabel('Speed Limit', fontsize = 16)\n",
    "plt.title('DISTRIBUCIÓ DELS LÍMITS DE VELOCITAT SEGONS\\nLA GRAVETAT DELS ACCIDENTS', fontsize = 17)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "#plt.savefig('gqtql_RespSpeed_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Accident_Severity i Number_of_Casualties\n",
    "dades['Number_of_Casualties'] = pd.Categorical(dades['Number_of_Casualties'], ordered=True)\n",
    "\n",
    "if '4' not in dades['Number_of_Casualties'].cat.categories:\n",
    "    dades['Number_of_Casualties'] = dades['Number_of_Casualties'].cat.add_categories('4')\n",
    "    \n",
    "dades.loc[dades['Number_of_Casualties'] == \"4 o més\", 'Number_of_Casualties'] = '4'\n",
    "\n",
    "categoria_a_eliminar = [\"4 o més\"]\n",
    "categories_presents = dades['Number_of_Casualties'].cat.categories\n",
    "categories_a_eliminar2 = [categoria for categoria in categoria_a_eliminar if categoria in categories_presents]\n",
    "\n",
    "if categories_a_eliminar2:\n",
    "    dades['Number_of_Casualties'] = dades['Number_of_Casualties'].cat.remove_categories(categories_a_eliminar2)\n",
    "\n",
    "dades['Number_of_Casualties'] = pd.to_numeric(dades['Number_of_Casualties'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.violinplot(x='Accident_Severity', y='Number_of_Casualties', data=dades, palette=[\"red\",\"orange\",\"green\"])\n",
    "plt.xlabel('Accident Severity')\n",
    "plt.ylabel('Number_of_Casualties')\n",
    "plt.title('DISTRIBUCIÓ DEL NOMBRE DE VÍCITTES SEGONS\\nLA GRAVETAT DELS ACCIDENTS', fontsize = 15)\n",
    "#plt.savefig('gqtql_RespCasualties_1.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "## Accident_Severity i Number_of_Vehicles\n",
    "\n",
    "dades['Number_of_Vehicles'] = pd.Categorical(dades['Number_of_Vehicles'], ordered=True)\n",
    "\n",
    "if '4' not in dades['Number_of_Vehicles'].cat.categories:\n",
    "    dades['Number_of_Vehicles'] = dades['Number_of_Vehicles'].cat.add_categories('4')\n",
    "    \n",
    "dades.loc[dades['Number_of_Vehicles'] == \"4 o més\", 'Number_of_Vehicles'] = \"4\"\n",
    "\n",
    "categoria_a_eliminar2 = [\"4 o més\"]\n",
    "categories_presents2 = dades['Number_of_Vehicles'].cat.categories\n",
    "categories_a_eliminar22 = [categoria for categoria in categoria_a_eliminar2 if categoria in categories_presents2]\n",
    "\n",
    "if categories_a_eliminar22:\n",
    "    dades['Number_of_Vehicles'] = dades['Number_of_Vehicles'].cat.remove_categories(categories_a_eliminar22)\n",
    "\n",
    "dades['Number_of_Vehicles'] = pd.to_numeric(dades['Number_of_Vehicles'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.violinplot(x='Accident_Severity', y='Number_of_Vehicles', data=dades, palette=[\"red\",\"orange\",\"green\"])\n",
    "plt.xlabel('Accident Severity')\n",
    "plt.ylabel('Number_of_Vehicles')\n",
    "plt.xticks(fontsize = 11)\n",
    "plt.yticks(fontsize = 11)\n",
    "plt.title('DISTRIBUCIÓ DEL NOMBRE DE VEHICLES SEGONS\\nLA GRAVETAT DELS ACCIDENTS', fontsize = 15)\n",
    "#plt.savefig('gqtql_RespNumVehicles_1.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "### Boxplot\n",
    "dades['Number_of_Vehicles'] = pd.to_numeric(dades['Number_of_Vehicles'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(x='Accident_Severity', y='Number_of_Vehicles', data=dades, palette=[\"red\",\"orange\",\"green\"])\n",
    "plt.xlabel('Accident Severity')\n",
    "plt.ylabel('Number of Vehicles')\n",
    "plt.xticks(fontsize = 11)\n",
    "plt.yticks(fontsize = 11)\n",
    "plt.title('DISTRIBUCIÓ DEL NOMBRE DE VEHICLES SEGONS\\nLA GRAVETAT DELS ACCIDENTS', fontsize = 15)\n",
    "#plt.savefig('gqtql_RespNumVehicles_1.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "## Accident_Severity i Mes_Accident \n",
    "\n",
    "dades['Mes_Accident'] = pd.to_numeric(dades['Mes_Accident'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.boxplot(x='Accident_Severity', y='Mes_Accident', data=dades, palette=[\"red\",\"orange\",\"green\"])\n",
    "plt.xlabel('Accident_Severity')\n",
    "plt.ylabel('Month')\n",
    "plt.xticks(fontsize = 11)\n",
    "plt.yticks(fontsize = 11)\n",
    "plt.title('DISTRIBUCIÓ DEL MES DELS ACCIDENTS SEGONS\\nLA DIFERENT GRAVETAT DELS ACCIDENTS', fontsize = 15)\n",
    "#plt.savefig('gqtql_RespMonth_1.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots discretitzats per Any_Accident\n",
    "\n",
    "dades['Speed_limit'] = pd.to_numeric(dades['Speed_limit'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.violinplot(x='Accident_Severity', y='Speed_limit', data=dades, hue = 'Any_Accident', \n",
    "               palette=[\"orange\",\"red\"], split=True)\n",
    "plt.xlabel('Accident Severity', fontsize = 23)\n",
    "plt.ylabel('Speed Limit', fontsize = 23)\n",
    "plt.xticks(fontsize = 23)\n",
    "plt.yticks(fontsize = 23)\n",
    "plt.title('LÍMITS DE VELOCITAT SEGONS LA GRAVETAT DELS\\nACCIDENTS PER ANY', fontsize = 25, pad = 80)\n",
    "plt.legend(title = 'Any',loc='upper center', bbox_to_anchor=(0.5, 1.18), fontsize = 21, title_fontsize = 21, ncol = 2)\n",
    "#plt.savefig('violin3_RespSpeedYear.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "dades['Number_of_Casualties'] = pd.to_numeric(dades['Number_of_Casualties'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.violinplot(x='Accident_Severity', y='Number_of_Casualties', data=dades, hue = 'Any_Accident', \n",
    "               palette=[\"orange\",\"red\"], split = True)\n",
    "plt.xlabel('Accident Severity', fontsize = 23)\n",
    "plt.ylabel('Number_of_Casualties', fontsize = 23)\n",
    "plt.xticks(fontsize = 23)\n",
    "plt.yticks(fontsize = 23)\n",
    "plt.title('DISTRIBUCIÓ DEL NOMBRE DE VÍCTIMES SEGONS\\nLA GRAVETAT DELS ACCIDENTS', fontsize = 25, pad = 80)\n",
    "plt.legend(title='Accident Year',loc='upper right', bbox_to_anchor=(0.75, 1.18), fontsize = 21, title_fontsize = 21, ncol = 2)\n",
    "#plt.savefig('violin3_RespCasualtiesYear.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "dades['Number_of_Vehicles'] = pd.to_numeric(dades['Number_of_Vehicles'], errors='coerce')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.violinplot(x='Accident_Severity', y='Number_of_Vehicles', data=dades, hue = \"Any_Accident\", \n",
    "               palette=[\"orange\",\"red\"], split=True)\n",
    "plt.xlabel('Accident Severity', fontsize = 23)\n",
    "plt.ylabel('Number_of_Vehicles', fontsize = 23)\n",
    "plt.xticks(fontsize = 23)\n",
    "plt.yticks(fontsize = 23)\n",
    "plt.title('DISTRIBUCIÓ DEL NOMBRE DE VEHICLES SEGONS\\nLA GRAVETAT DELS ACCIDENTS', fontsize = 25, pad = 80)\n",
    "plt.legend(title='Accident Year',loc='upper right', bbox_to_anchor=(0.75, 1.18), fontsize = 21, title_fontsize = 21, ncol = 2)\n",
    "#plt.savefig('violin3_RespNumVehiclesYear.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ecf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots discretitzats - hue = Number_of_casualties\n",
    "\n",
    "dades_new['NumberCasualties2'] = dades['Number_of_Casualties'].apply(lambda x: \"1\" if x == 1 else \"Més de 1\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.violinplot(x='Accident_Severity', y='Speed_limit', data=dades_new, hue='NumberCasualties2', \n",
    "               palette = [\"orange\",\"red\"], split=True)\n",
    "plt.title('LÍMITS DE VELOCITAT SEGONS LA GRAVETAT DELS\\nACCIDENTS PER NOMBRE DE VÍCTIMES', fontsize = 25, pad = 85)\n",
    "plt.xlabel('Accident Severity', fontsize = 23)\n",
    "plt.ylabel('Speed limit', fontsize = 23)\n",
    "plt.xticks(fontsize = 23)\n",
    "plt.yticks(fontsize = 23)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.legend(title='Number of Casualties',loc='upper center', bbox_to_anchor=(0.5, 1.18), fontsize = 21, title_fontsize = 21, ncol = 2)\n",
    "#plt.savefig('violin3_RespSpeedCasualt_1.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "dades_new['Number_of_Vehicles'] = dades_new['Number_of_Vehicles'].replace('4 o més', 4)\n",
    "sns.violinplot(x='Accident_Severity', y='Number_of_Vehicles', data=dades_new, hue = \"NumberCasualties2\", \n",
    "               palette=[\"orange\",\"red\"], split=True)\n",
    "plt.xlabel('Accident Severity', fontsize = 23)\n",
    "plt.ylabel('Number of Vehicles', fontsize = 23)\n",
    "plt.xticks(fontsize = 23)\n",
    "plt.yticks(fontsize = 23)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('NOMBRE DE VEHICLES SEGONS LA GRAVETAT DELS ACCIDENTS\\nPER NOMBRE DE VÍCTIMES', fontsize = 25, pad = 85)\n",
    "plt.legend(title='Number of Casualties',loc='upper right', bbox_to_anchor=(0.75, 1.18), fontsize = 21, title_fontsize = 21, ncol = 2)\n",
    "#plt.savefig('gqtql_RespNumVehicles_1.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e6461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series temporals\n",
    "\n",
    "mesos = {\n",
    "    1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31\n",
    "}\n",
    "\n",
    "mitjanes_per_mes_total = {}\n",
    "\n",
    "classes = {\n",
    "    'Fatal': 'Fatal',\n",
    "    'Serious': 'Serious',\n",
    "    'Slight': 'Slight'\n",
    "}\n",
    "\n",
    "for i, cond in classes.items():\n",
    "    df = dades[dades['Accident_Severity'] == cond]\n",
    "    \n",
    "    mitjanes_per_mes = []\n",
    "    \n",
    "    for any_accident in [2021, 2022]:\n",
    "        df_any = df[df['Any_Accident'] == any_accident]\n",
    "        \n",
    "        for mes in range(1, 13):\n",
    "            df_mes = df_any[df_any['Mes_Accident'] == mes]\n",
    "            \n",
    "            num_dies = mesos[mes]\n",
    "            mitjana_casos = len(df_mes) / num_dies\n",
    "            \n",
    "            mitjanes_per_mes.append(mitjana_casos)\n",
    "            \n",
    "    mitjanes_per_mes_total[i] = mitjanes_per_mes\n",
    "    \n",
    "mitjanes_per_mes_fatal = mitjanes_per_mes_total['Fatal']\n",
    "mitjanes_per_mes_serious = mitjanes_per_mes_total['Serious']\n",
    "mitjanes_per_mes_slight = mitjanes_per_mes_total['Slight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758fd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FATAL\n",
    "fatal_data = dades[dades['Accident_Severity'] == 'Fatal']\n",
    "monthly_means = pd.DataFrame(mitjanes_per_mes_fatal, columns=['Mean Fatal Accidents'])\n",
    "\n",
    "monthly_index = pd.date_range(start='2021-01-01', periods=len(monthly_means), freq='M')\n",
    "monthly_means.index = monthly_index - pd.offsets.MonthEnd()\n",
    "\n",
    "daily_counts = fatal_data.groupby('Accident Date').size()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "daily_counts.plot(kind='line', marker='', color='gray', label='Fatal Accidents per Day')\n",
    "plt.plot(monthly_means.index, monthly_means['Mean Fatal Accidents'], marker='o', color='red', \n",
    "         linestyle='-', label='Monthly Mean Fatal Accidents')\n",
    "first_day_of_each_month = [pd.Timestamp(year, month, 1) for year in range(2021, 2023) for month in range(1, 13)]\n",
    "\n",
    "plt.xticks(first_day_of_each_month, rotation=90, fontsize = 14)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Accidents Fatals', fontsize = 15)\n",
    "plt.title('EVOLUCIÓ DIÀRIA D\\'ACCIDENTS FATALS AMB MITJANA MENSUAL', fontsize=17)\n",
    "plt.legend(loc='upper right', fontsize = 14, title_fontsize = 14)\n",
    "plt.ylim(0,20)\n",
    "plt.savefig('gqtql_TendenciaFatal_1.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# SERIOUS\n",
    "serious_data = dades[dades['Accident_Severity'] == 'Serious']\n",
    "monthly_means = pd.DataFrame(mitjanes_per_mes_serious, columns=['Mean Serious Accidents'])\n",
    "\n",
    "monthly_index = pd.date_range(start='2021-01-01', periods=len(monthly_means), freq='M')\n",
    "monthly_means.index = monthly_index - pd.offsets.MonthEnd()\n",
    "\n",
    "daily_counts = serious_data.groupby('Accident Date').size()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "daily_counts.plot(kind='line', marker='', color='gray', label='Serious Accidents per Day')\n",
    "plt.plot(monthly_means.index, monthly_means['Mean Serious Accidents'], marker='o', color='darkorange', \n",
    "         linestyle='-', label='Monthly Mean Serious Accidents')\n",
    "\n",
    "first_day_of_each_month = [pd.Timestamp(year, month, 1) for year in range(2021, 2023) for month in range(1, 13)]\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.xticks(first_day_of_each_month, rotation=90, fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Accidents Greus', fontsize = 15)\n",
    "plt.title('EVOLUCIÓ DIÀRIA D\\'ACCIDENTS GREUS AMB MITJANA MENSUAL', fontsize=17)\n",
    "plt.legend(loc='upper right', fontsize = 14, title_fontsize = 14)\n",
    "\n",
    "\n",
    "extended_start_date = monthly_index[0] - pd.DateOffset(months=2)\n",
    "extended_end_date = monthly_index[-1] + pd.DateOffset(months=1)\n",
    "plt.xlim(extended_start_date, extended_end_date) \n",
    "plt.ylim(0,100)\n",
    "plt.savefig('gqtql_TendenciaSerious_1.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# SLIGHT\n",
    "slight_data = dades[dades['Accident_Severity'] == 'Slight']\n",
    "monthly_means = pd.DataFrame(mitjanes_per_mes_slight, columns=['Mean Slight Accidents'])\n",
    "\n",
    "monthly_index = pd.date_range(start='2021-01-01', periods=len(monthly_means), freq='M')\n",
    "monthly_means.index = monthly_index - pd.offsets.MonthEnd()\n",
    "\n",
    "daily_counts = slight_data.groupby('Accident Date').size()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "daily_counts.plot(kind='line', marker='', color='gray', label='Slight Accidents per Day')\n",
    "plt.plot(monthly_means.index, monthly_means['Mean Slight Accidents'], marker='o', color='green', \n",
    "         linestyle='-', label='Monthly Mean Slight Accidents')\n",
    "first_day_of_each_month = [pd.Timestamp(year, month, 1) for year in range(2021, 2023) for month in range(1, 13)]\n",
    "\n",
    "plt.xticks(first_day_of_each_month, [date.strftime('%Y-%m-%d') for date in first_day_of_each_month], rotation=90, fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Accidents Lleus', fontsize = 15)\n",
    "plt.title('EVOLUCIÓ DIÀRIA D\\'ACCIDENTS LLEUS AMB MITJANA MENSUAL', fontsize=17)\n",
    "plt.legend(loc='upper right', fontsize = 14, title_fontsize = 14)\n",
    "\n",
    "extended_start_date = monthly_index[0] - pd.DateOffset(months=2)\n",
    "extended_end_date = monthly_index[-1] + pd.DateOffset(months=1)\n",
    "plt.xlim(extended_start_date, extended_end_date) \n",
    "plt.ylim(0,650)\n",
    "plt.savefig('gqtql_TendenciaSlight_1.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Fatal + Serious + Slight)\n",
    "daily_counts = dades.groupby([dades['Accident Date'].dt.year, dades['Accident Date'].dt.month, dades['Accident Date'].dt.day, 'Accident_Severity']).size().unstack()\n",
    "plt.figure(figsize=(12, 6))\n",
    "daily_counts.plot(kind='line', marker='', color=['red', 'orange', 'green'])\n",
    "\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Accidents')\n",
    "plt.title('EVOLUCIÓ DE LA GRAVETAT DELS ACCIDENTS AL LARG DEL TEMPS')\n",
    "plt.legend(title='Accident Severity', loc='upper right', bbox_to_anchor=(1.4, 1))\n",
    "plt.xticks(rotation=90, fontsize = 9)\n",
    "plt.yticks(fontsize = 9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002f36dc",
   "metadata": {},
   "source": [
    "####  3.2 ANÀLISI DESCRIPTIVA QUALITATIVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16813c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariant\n",
    "\n",
    "# Accident_Severity \n",
    "frequencia_accident_sev = dades['Accident_Severity'].value_counts().sort_index()\n",
    "colors = ['red','orange','green']\n",
    "start_angles = 40\n",
    "plt.pie(frequencia_accident_sev.values, labels=frequencia_accident_sev.index, autopct='%1.1f%%',\n",
    "        startangle=start_angles, textprops={'fontsize': 12, 'color': 'white'}, labeldistance=None, colors=colors)\n",
    "plt.title('FREQÜÈNCIA DE LA GRAVETAT DELS ACCIDENTS', fontsize = 15)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1), prop={'size': 12})\n",
    "#plt.savefig('gq_resp_1')\n",
    "plt.show()\n",
    "\n",
    "# Weather_Conditions\n",
    "frequencia_weather = dades['Weather_Conditions'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(frequencia_weather.index, frequencia_weather.values, color = '#008000', width = 0.9)\n",
    "plt.xlabel('Weather Conditions', fontsize = 11)\n",
    "plt.ylabel('Freqüència', fontsize = 11)\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS PER LES DIFERENTS\\nCONDICIONS CLIMÀTIQUES', fontsize = 15)\n",
    "plt.xticks(rotation=60, fontsize=11)\n",
    "plt.yticks(fontsize = 11)\n",
    "for i, v in zip(frequencia_weather.index, frequencia_weather.values):\n",
    "    plt.text(i , v + 0.1, str(v), ha='center', va='bottom', color='black', fontsize=8)\n",
    "#plt.savefig('gq_weather_1.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Light_Conditions\n",
    "frequencia_light = dades['Light_Conditions'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(frequencia_light.index, frequencia_light.values, color = '#008000', width = 0.9)\n",
    "plt.xlabel('Light Conditions', fontsize = 11)\n",
    "plt.ylabel('Freqüència', fontsize = 11)\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS PER LES DIFERENTS\\nCONDICIONS DE LLUM', fontsize = 15)\n",
    "plt.xticks(rotation=0, fontsize = 8)\n",
    "plt.yticks(fontsize = 11)\n",
    "for i, v in zip(frequencia_light.index, frequencia_light.values):\n",
    "    plt.text(i , v + 0.1, str(v), ha='center', va='bottom', color='black', fontsize=8)\n",
    "#plt.savefig('gq_light_1')\n",
    "plt.show()\n",
    "\n",
    "# Local_Authority_(District)\n",
    "top_20_districts = dades['Local_Authority_(District)'].value_counts().nlargest(20)\n",
    "\n",
    "percentatges = (top_20_districts / top_20_districts.sum()) * 100\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(top_20_districts.index, percentatges, color='#008000', width=0.9)\n",
    "plt.xlabel('20 principals autoritats locals', fontsize = 11)\n",
    "plt.ylabel('Percentatge (%)')\n",
    "plt.title('DISTRIBUCIÓ PERCENTUAL DE LES 20 PRINCIPALS\\nENTITATS LOCALS')\n",
    "plt.xticks(rotation=60, fontsize=8)\n",
    "for bar, percentage in zip(bars, percentatges):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.1, \n",
    "             f'{percentage:.1f}%', ha='center', va='bottom', color='black', fontsize=7)\n",
    "#plt.savefig('gq_district_1')\n",
    "plt.show()\n",
    "\n",
    "# Vehicle_Type\n",
    "frequencia_vehicletype = dades['Vehicle_Type'].value_counts().sort_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(frequencia_vehicletype.index, frequencia_vehicletype.values, color = '#008000', width = 0.9)\n",
    "plt.xlabel('Vehicle Type')\n",
    "plt.ylabel('Freqüència')\n",
    "plt.title('FREQÜÈNCIA D\\'ACCIDENTS PELS DIFERENTS TIPUS\\nDE VEHICLES', fontsize = 15)\n",
    "plt.xticks(rotation=90, fontsize=8)\n",
    "for i, v in zip(frequencia_vehicletype.index, frequencia_vehicletype.values):\n",
    "    plt.text(i , v + 0.1, str(v), ha='center', va='bottom', color='black', fontsize=7)\n",
    "#plt.savefig('gq_vehicle_1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dccdc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariant\n",
    "\n",
    "# Accident_Severity i Day_of_Week\n",
    "cross_table = pd.crosstab(dades['Day_of_Week'], dades['Accident_Severity'])\n",
    "order_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "cross_table = cross_table.reindex(order_days)\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cross_table.plot(kind='bar', stacked=True, color=['red', 'orange', 'green'], ax=ax)\n",
    "\n",
    "plt.legend(title='Accident Severity', bbox_to_anchor=(1.15, 0.6), loc='upper left', fontsize = 15, title_fontsize = 15)\n",
    "plt.xlabel('Day of Week', fontsize=15)\n",
    "plt.xticks(rotation = 0, fontsize=15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('PROPORCIÓ DE LA GRAVETAT DELS ACCIDENTS\\nI FREQÜÈNCIA PER DIA', fontsize = 17)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "frequencies = dades['Day_of_Week'].value_counts().reindex(order_days)\n",
    "ax2.plot(frequencies.index, frequencies.values, color='black', linestyle='-', marker='o', label='Freqüència d\\'accidents')\n",
    "ax2.tick_params(axis='y', labelsize=15)\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.55, 0.7), fontsize = 15)\n",
    "\n",
    "#plt.savefig('gqq_RespDay_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Accident_Severity i Road_Surface_Conditions\n",
    "cross_table = pd.crosstab(dades['Road_Surface_Conditions'], dades['Accident_Severity'])\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "\n",
    "frequencies = dades['Road_Surface_Conditions'].value_counts()\n",
    "frequencies = frequencies.reindex(cross_table.index) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cross_table.plot(kind='bar', stacked=True, color=['red', 'orange', 'green'], ax=ax)\n",
    "\n",
    "plt.legend(title='Accident Severity', bbox_to_anchor=(1.2, 0.6), loc='upper left', fontsize = 15, title_fontsize = 15)\n",
    "plt.xlabel('Road Surface Conditions', fontsize=14)\n",
    "plt.xticks(ticks=[0, 1, 2, 3, 4], labels=['Dry', 'Flood over 3cm', 'Frost or ice', 'Snow', 'Wet or damp'], \n",
    "           fontsize=14, rotation = 0)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.title('PROPORCIÓ DE LA GRAVETAT DELS ACCIDENTS I\\nFREQÜÈNCIA PER CONDICIONS DE SUPERFÍCIE', fontsize=17)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(frequencies.index, frequencies.values, color='black', linestyle='-', marker='o', label='Freqüència d\\'accidents')\n",
    "ax2.tick_params(axis='y', labelsize=14)\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.5, 0.7), fontsize = 15)\n",
    "\n",
    "#plt.savefig('gqq_RespSurface_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Accident_Severity i Weather_Conditions\n",
    "cross_table = pd.crosstab(dades['Weather_Conditions'], dades['Accident_Severity'])\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "\n",
    "frequencies = dades['Weather_Conditions'].value_counts()\n",
    "frequencies = frequencies.reindex(cross_table.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cross_table.plot(kind='bar', stacked=True, color=['red', 'orange', 'green'], ax=ax)\n",
    "\n",
    "plt.legend(title='Accident Severity', bbox_to_anchor=(1.15, 0.6), loc='upper left', fontsize = 14, title_fontsize = 14)\n",
    "plt.xlabel('Weather Conditions', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.title('PROPORCIÓ DE LA GRAVETAT DELS ACCIDENTS I\\nFREQÜÈNCIA PER CONDICIONS CLIMÀTIQUES', fontsize = 15)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(frequencies.index, frequencies.values, color='black', linestyle='-', marker='o', label='Freqüència d\\'accidents')\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.5, 0.7), fontsize = 14)\n",
    "\n",
    "#plt.savefig('gqq_RespWeather_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Accident_Severity i Urban or Rural Area\n",
    "cross_table = pd.crosstab(dades['Urban_or_Rural_Area'], dades['Accident_Severity'])\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "\n",
    "frequencies = dades['Urban_or_Rural_Area'].value_counts()\n",
    "frequencies = frequencies.reindex(cross_table.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "cross_table.plot(kind = 'bar', stacked = True, color = ['red', 'orange', 'green'], ax = ax)\n",
    "\n",
    "plt.legend(title='Accident Severity', bbox_to_anchor=(1.15, 0.6), loc='upper left', fontsize = 15, title_fontsize = 15)\n",
    "plt.xlabel('Urban_or_Rural_Area', fontsize = 15)\n",
    "plt.xticks(rotation = 0, fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.title('PROPORCIÓ DE LA GRAVETAT DELS ACCIDENTS I\\nFREQÜÈNCIA PER ÀREES', fontsize = 17)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(frequencies.index, frequencies.values, color='black', linestyle='-', marker='o', label='Freqüència d\\'accidents')\n",
    "ax2.tick_params(axis='y', labelsize=15)\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.5, 0.7), fontsize = 15)\n",
    "\n",
    "#plt.savefig('gqq_RespUrbRur_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Accident_Severity i Road Type\n",
    "cross_table = pd.crosstab(dades['Road_Type'], dades['Accident_Severity'])\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "\n",
    "frequencies = dades['Road_Type'].value_counts()\n",
    "frequencies = frequencies.reindex(cross_table.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cross_table.plot(kind='bar', stacked=True, color=['red', 'orange', 'green'], ax=ax)\n",
    "\n",
    "plt.legend(title='Accident Severity', bbox_to_anchor=(1.15, 0.6), loc='upper left', fontsize = 14, title_fontsize = 14)\n",
    "plt.xlabel('Road_Type', fontsize=14)\n",
    "plt.xticks(rotation = 0, fontsize=12)\n",
    "plt.title('PROPORCIÓ DE LA GRAVETAT DELS ACCIDENTS I\\nFREQÜÈNCIA PER TIPUS DE CARRETERA', fontsize = 15)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(frequencies.index, frequencies.values, color='black', linestyle='-', marker='o', label='Freqüència d\\'accidents')\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.5, 0.7), fontsize = 14)\n",
    "\n",
    "#plt.savefig('gqq_RespRoad_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Accident_Severity i Vehicle Type\n",
    "cross_table = pd.crosstab(dades['Vehicle_Type'], dades['Accident_Severity'])\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "\n",
    "frequencies = dades['Vehicle_Type'].value_counts()\n",
    "frequencies = frequencies.reindex(cross_table.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cross_table.plot(kind='bar', stacked=True, color=['red', 'orange', 'green'], ax=ax)\n",
    "\n",
    "plt.legend(title='Accident Severity', bbox_to_anchor=(1.15, 0.6), loc='upper left', fontsize = 14, title_fontsize = 14)\n",
    "plt.xlabel('Vehicle_Type', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.title('PROPORCIÓ DE LA GRAVETAT DELS ACCIDENTS I\\nFREQÜÈNCIA PER TIPUS DE VEHÍCLE', fontsize = 15)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(frequencies.index, frequencies.values, color='black', linestyle='-', marker='o', label='Freqüència d\\'accidents')\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.5, 0.7), fontsize = 14)\n",
    "\n",
    "#plt.savefig('gqq_RespVehicle_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Accident_Severity i Junction_Control\n",
    "cross_table = pd.crosstab(dades['Junction_Control'], dades['Accident_Severity'])\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "\n",
    "frequencies = dades['Junction_Control'].value_counts()\n",
    "frequencies = frequencies.reindex(cross_table.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cross_table.plot(kind='bar', stacked=True, color=['red', 'orange', 'green'], ax=ax)\n",
    "\n",
    "plt.legend(title='Accident Severity', bbox_to_anchor=(1.15, 0.6), loc='upper left', fontsize = 14, title_fontsize = 14)\n",
    "plt.xlabel('Junction Control', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.title('PROPORCIÓ DE LA GRAVETAT DELS ACCIDENTS I\\nFREQÜÈNCIA PER TIPUS DE REGULACIÓ EN L\\'ENCREUAMENT', fontsize = 15)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(frequencies.index, frequencies.values, color='black', linestyle='-', marker='o', label='Freqüència d\\'accidents')\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.5, 0.7), fontsize = 14)\n",
    "\n",
    "#plt.savefig('gqq_RespJControl_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Accident_Severity i Junction_Detail\n",
    "cross_table = pd.crosstab(dades['Junction_Detail'], dades['Accident_Severity'])\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "\n",
    "frequencies = dades['Junction_Detail'].value_counts()\n",
    "frequencies = frequencies.reindex(cross_table.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cross_table.plot(kind='bar', stacked=True, color=['red', 'orange', 'green'], ax=ax)\n",
    "\n",
    "plt.legend(title='Accident Severity', bbox_to_anchor=(1.15, 0.6), loc='upper left', fontsize = 14, title_fontsize = 14)\n",
    "plt.xlabel('Junction Detail', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.title('PROPORCIÓ DE LA GRAVETAT DELS ACCIDENTS I\\nFREQÜÈNCIA PER TIPUS D\\'ENCREUAMENT', fontsize = 15)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(frequencies.index, frequencies.values, color='black', linestyle='-', marker='o', label='Freqüència d\\'accidents')\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.5, 0.7), fontsize = 14)\n",
    "\n",
    "#plt.savefig('gqq_RespJDetail_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Accident_Severity i Light_Conditions\n",
    "cross_table = pd.crosstab(dades['Light_Conditions'], dades['Accident_Severity'])\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "\n",
    "frequencies = dades['Light_Conditions'].value_counts()\n",
    "frequencies = frequencies.reindex(cross_table.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cross_table.plot(kind='bar', stacked=True, color=['red', 'orange', 'green'], ax=ax)\n",
    "\n",
    "plt.legend(title='Accident Severity', bbox_to_anchor=(1.15, 0.6), loc='upper left', fontsize = 14, title_fontsize = 14)\n",
    "plt.xlabel('Light Conditions', fontsize = 13)\n",
    "plt.xticks(rotation = 0, fontsize=11)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.title('PROPORCIÓ DE LA GRAVETAT DELS ACCIDENTS I\\nFREQÜÈNCIA PER DIFERENTS CONDICIONS DE LLUM', fontsize = 17)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(frequencies.index, frequencies.values, color='black', linestyle='-', marker='o', label='Freqüència d\\'accidents')\n",
    "ax2.tick_params(axis='y', labelsize=13)\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.5, 0.7), fontsize = 14)\n",
    "\n",
    "#plt.savefig('gqq_RespLight_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Accident_Severity i Carriageway_Hazards\n",
    "cross_table = pd.crosstab(dades['Carriageway_Hazards'], dades['Accident_Severity'])\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "\n",
    "frequencies = dades['Carriageway_Hazards'].value_counts()\n",
    "frequencies = frequencies.reindex(cross_table.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cross_table.plot(kind='bar', stacked=True, color=['red', 'orange', 'green'], ax=ax)\n",
    "\n",
    "plt.legend(title='Accident Severity', bbox_to_anchor=(1.15, 0.6), loc='upper left', fontsize = 14, title_fontsize = 14)\n",
    "plt.xlabel('Carriageway Hazards', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.title('PROPORCIÓ DE LA GRAVETAT DELS ACCIDENTS I\\nFREQÜÈNCIA PER DIFERENTS PERILLS EN LA CALÇADA', fontsize = 15)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(frequencies.index, frequencies.values, color='black', linestyle='-', marker='o', label='Freqüència d\\'accidents')\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.5, 0.7), fontsize = 14)\n",
    "\n",
    "#plt.savefig('gqq_RespCarriage_2.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Accident_Severity i Police_Force\n",
    "cross_table = pd.crosstab(dades['Police_Force'], dades['Accident_Severity'])\n",
    "cross_table = cross_table.div(cross_table.sum(axis=1), axis=0)\n",
    "\n",
    "frequencies = dades['Police_Force'].value_counts()\n",
    "frequencies = frequencies.reindex(cross_table.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cross_table.plot(kind='bar', stacked=True, color=['red', 'orange', 'green'], ax=ax)\n",
    "\n",
    "plt.legend(title='Accident Severity', bbox_to_anchor=(1.15, 0.6), loc='upper left', fontsize = 14, title_fontsize = 14)\n",
    "plt.xlabel('Police Force', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.title('PROPORCIÓ DE LA GRAVETAT DELS ACCIDENTS I\\nFREQÜÈNCIA PER LES DIFERENTS FORCES POLICIALS', fontsize = 15)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(frequencies.index, frequencies.values, color='black', linestyle='-', marker='o', label='Freqüència d\\'accidents')\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.5, 0.7), fontsize = 14)\n",
    "\n",
    "#plt.savefig('gqq_RespPolice_2.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freqüències relatives (FATAL)\n",
    "dades_serious = dades[dades['Accident_Severity'] == 'Fatal']\n",
    "total_accidents_serious = len(dades_serious)\n",
    "\n",
    "freq_relatives = dades_serious.groupby(['Latitude', 'Longitude']).size() / total_accidents_serious\n",
    "freq_relatives = freq_relatives.reset_index(name='Freqüència relativa')\n",
    "\n",
    "escalem = MinMaxScaler()\n",
    "freq_relatives[['Freqüència relativa']] = escalem.fit_transform(freq_relatives[['Freqüència relativa']])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'latitude': freq_relatives['Latitude'],\n",
    "    'longitude': freq_relatives['Longitude'],\n",
    "    'F. Rel': freq_relatives['Freqüència relativa']\n",
    "})\n",
    "\n",
    "fig = px.density_mapbox(df, lat='latitude', lon='longitude', z='F. Rel',\n",
    "                        radius=20,\n",
    "                        center=dict(lat=min(df.latitude), lon=min(df.longitude)),\n",
    "                        zoom=6,\n",
    "                        mapbox_style='open-street-map',\n",
    "                        height=1000,\n",
    "                        width=1000)\n",
    "\n",
    "fig.update_layout(\n",
    "    coloraxis_colorbar=dict(\n",
    "        title='F. Rel',  \n",
    "        titlefont=dict(size=35, color='black'),  \n",
    "        tickfont=dict(size=35, color='black') \n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freqüències relatives (SERIOUS)\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dades_serious = dades[dades['Accident_Severity'] == 'Serious']\n",
    "total_accidents_serious = len(dades_serious)\n",
    "\n",
    "freq_relatives = dades_serious.groupby(['Latitude', 'Longitude']).size() / total_accidents_serious\n",
    "freq_relatives = freq_relatives.reset_index(name='Freqüència relativa')\n",
    "\n",
    "escalem = MinMaxScaler()\n",
    "freq_relatives[['Freqüència relativa']] = escalem.fit_transform(freq_relatives[['Freqüència relativa']])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'latitude': freq_relatives['Latitude'],\n",
    "    'longitude': freq_relatives['Longitude'],\n",
    "    'F. Rel': freq_relatives['Freqüència relativa']\n",
    "})\n",
    "\n",
    "fig = px.density_mapbox(df, lat='latitude', lon='longitude', z='F. Rel',\n",
    "                        radius=12,\n",
    "                        center=dict(lat=min(df.latitude), lon=min(df.longitude)),\n",
    "                        zoom=6,\n",
    "                        mapbox_style='open-street-map',\n",
    "                        height=1000,\n",
    "                        width=1000)\n",
    "\n",
    "fig.update_layout(\n",
    "    coloraxis_colorbar=dict(\n",
    "        title='F. Rel',  \n",
    "        titlefont=dict(size=35, color='black'),  \n",
    "        tickfont=dict(size=35, color='black') \n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95920617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freqüències relatives (SLIGHT)\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dades_slight = dades[dades['Accident_Severity'] == 'Slight']\n",
    "total_accidents_slight = len(dades_slight)\n",
    "\n",
    "freq_relatives = dades_slight.groupby(['Latitude', 'Longitude']).size() / total_accidents_slight\n",
    "freq_relatives = freq_relatives.reset_index(name='Freqüència relativa')\n",
    "\n",
    "escalem = MinMaxScaler()\n",
    "freq_relatives[['Freqüència relativa']] = escalem.fit_transform(freq_relatives[['Freqüència relativa']])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'latitude': freq_relatives['Latitude'],\n",
    "    'longitude': freq_relatives['Longitude'],\n",
    "    'F. Rel': freq_relatives['Freqüència relativa']\n",
    "})\n",
    "\n",
    "fig = px.density_mapbox(df, lat='latitude', lon='longitude', z='F. Rel',\n",
    "                        radius=12,\n",
    "                        center=dict(lat=min(df.latitude), lon=min(df.longitude)),\n",
    "                        zoom=6,\n",
    "                        mapbox_style='open-street-map',\n",
    "                        height=1000,\n",
    "                        width=1000)\n",
    "\n",
    "fig.update_layout(\n",
    "    coloraxis_colorbar=dict(\n",
    "        title='F. Rel',  \n",
    "        titlefont=dict(size=35, color='black'),  \n",
    "        tickfont=dict(size=35, color='black') \n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee20a3",
   "metadata": {},
   "source": [
    "Correlacions i associacions entre variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf058ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriu de correlacions (Variables numèriques)\n",
    "cmap = sns.diverging_palette(10, 150, s=100, l=45, sep=5, as_cmap=True,)\n",
    "\n",
    "variables_quantitatives = ['Latitude', 'Longitude','Number_of_Casualties', 'Number_of_Vehicles', 'Speed_limit']\n",
    "matriu_correlacions = dades[variables_quantitatives].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "cmap_custom = sns.color_palette(\"Greens\", as_cmap=True)\n",
    "heatmap = sns.heatmap(matriu_correlacions, annot=True, cmap=cmap, fmt=\".2f\", linewidths=0.5, center=0,  \n",
    "                      annot_kws={\"fontsize\":12})\n",
    "\n",
    "for _, spine in heatmap.spines.items():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_color('white')\n",
    "\n",
    "heatmap.set_title('MATRIU DE CORRELACIONS', fontsize = 15)\n",
    "plt.xticks(rotation=90, fontsize=11)\n",
    "plt.yticks(rotation=0, fontsize=11)\n",
    "\n",
    "#plt.savefig('CorrNumeriques.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIANGLE SUPERIOR - altres tonalitats\n",
    "cmap = sns.diverging_palette(10, 150, s=100, l=45, sep=5, as_cmap=True,)\n",
    "\n",
    "variables_quantitatives = ['Latitude', 'Longitude','Number_of_Casualties', 'Number_of_Vehicles', 'Speed_limit', 'Any_Accident']\n",
    "matriu_correlacions = dades[variables_quantitatives].corr()\n",
    "triangle_superior_correlacions = matriu_correlacions.where(np.triu(np.ones(matriu_correlacions.shape), k = 0).astype(bool))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "cmap_custom = sns.color_palette(\"RdYlGn\", as_cmap=True)\n",
    "heatmap = sns.heatmap(triangle_superior_correlacions, annot = True, cmap = cmap_custom, fmt=\".2f\", linewidths = 0.5, \n",
    "                      center = 0,  annot_kws = {\"fontsize\":20})\n",
    "\n",
    "for _, spine in heatmap.spines.items():\n",
    "    spine.set_visible(True)\n",
    "    spine.set_color('white')\n",
    "\n",
    "heatmap.set_title('MATRIU DE CORRELACIONS', fontsize = 24)\n",
    "plt.xticks(rotation = 90, fontsize = 21)\n",
    "plt.yticks(rotation = 0, fontsize = 21)\n",
    "plt.savefig('CorrNumeriques2.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8faab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V Cramer - Associacions entre variables\n",
    "dades_qualitatives = dades[[\"Day_of_Week\", \"Junction_Control\", \"Junction_Detail\", \"Accident_Severity\", \"Light_Conditions\", \n",
    "                            \"Carriageway_Hazards\", \"Police_Force\", \"Road_Surface_Conditions\", \"Road_Type\", \n",
    "                           \"Urban_or_Rural_Area\",\"Weather_Conditions\", \"Vehicle_Type\",\"Local_Authority_(District)\"]]\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    contingency_table = pd.crosstab(x, y)\n",
    "    chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "    n = contingency_table.sum().sum()\n",
    "    k = max(contingency_table.shape)\n",
    "    return np.sqrt(chi2 / (n * (k - 1)))\n",
    "\n",
    "n_variables = len(dades_qualitatives.columns)\n",
    "cramer_matrix = np.eye(n_variables)\n",
    "\n",
    "for i in range(n_variables):\n",
    "    for j in range(i + 1, n_variables):\n",
    "        v_cramer = cramers_v(dades_qualitatives.iloc[:, i], dades_qualitatives.iloc[:, j])\n",
    "        cramer_matrix[i, j] = v_cramer\n",
    "        cramer_matrix[j, i] = v_cramer  \n",
    "\n",
    "#cmap_custom = sns.color_palette(\"Greens\", as_cmap=True)\n",
    "cmap_custom = sns.diverging_palette(10, 150, as_cmap=True, s=100, l=45, sep=5)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cramer_matrix, annot=True, cmap=cmap_custom, center=0, fmt=\".2f\", linewidths=0.5, vmin=0, vmax=1)\n",
    "\n",
    "plt.title('V DE CRAMER - ASSOCIACIÓ ENTRE VARIABLES CATEGÒRIQUES', fontsize = 15)\n",
    "plt.xticks(np.arange(n_variables) + 0.5, dades_qualitatives.columns, rotation=90, fontsize=11)\n",
    "plt.yticks(np.arange(n_variables) + 0.5, dades_qualitatives.columns, rotation=0, fontsize=11)\n",
    "\n",
    "#plt.savefig('AssociCategoriques.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c25755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIANGLE SUPERIOR - altres tonalitats\n",
    "dades_qualitatives = dades[[\"Day_of_Week\", \"Junction_Control\", \"Junction_Detail\", \"Accident_Severity\", \"Light_Conditions\", \n",
    "                            \"Carriageway_Hazards\", \"Police_Force\", \"Road_Surface_Conditions\", \"Road_Type\", \n",
    "                           \"Urban_or_Rural_Area\",\"Weather_Conditions\", \"Vehicle_Type\",\"Local_Authority_(District)\"]]\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    contingency_table = pd.crosstab(x, y)\n",
    "    chi2, _, _, _ = chi2_contingency(contingency_table)\n",
    "    n = contingency_table.sum().sum()\n",
    "    k = max(contingency_table.shape)\n",
    "    return np.sqrt(chi2 / (n * (k - 1)))\n",
    "\n",
    "n_variables = len(dades_qualitatives.columns)\n",
    "matriu_cramer = np.eye(n_variables)\n",
    "\n",
    "for i in range(n_variables):\n",
    "    for j in range(i + 1, n_variables):\n",
    "        v_cramer = cramers_v(dades_qualitatives.iloc[:, i], dades_qualitatives.iloc[:, j])\n",
    "        matriu_cramer[i, j] = v_cramer\n",
    "        matriu_cramer[j, i] = v_cramer  \n",
    "\n",
    "cmap_custom = sns.color_palette(\"RdYlGn\", as_cmap=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(triangle_superior_vcramer, annot=True, cmap=cmap_custom, center=0, fmt=\".2f\", linewidths=0.5, vmin=0, vmax=1)\n",
    "\n",
    "plt.title('V DE CRAMER - ASSOCIACIÓ ENTRE VARIABLES CATEGÒRIQUES', fontsize = 17)\n",
    "plt.xticks(np.arange(n_variables) + 0.5, dades_qualitatives.columns, rotation=90, fontsize=14)\n",
    "plt.yticks(np.arange(n_variables) + 0.5, dades_qualitatives.columns, rotation=0, fontsize=14)\n",
    "plt.savefig('AssociCategoriques2.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a369475",
   "metadata": {},
   "source": [
    "### 4. PASSOS PREVIS AL MODELATGE\n",
    "#### 4.1 TRACTAMENT DELS VALORS PERDUTS (NA)\n",
    "\n",
    "**Variables amb NA**\n",
    "\n",
    "- Carriageway_Hazards\n",
    "- Road_Surface_Conditions\n",
    "- Road_Type\n",
    "- Time\n",
    "- Wheather_Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c1e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time - imputació\n",
    "## darkness - no lighting\n",
    "from statistics import mode\n",
    "darknessnolight = dades[dades['Light_Conditions'] == \"Darkness - no lighting\"].copy()\n",
    "darknessnolight['Time'] = pd.to_datetime(darknessnolight['Time'], format='%H:%M')\n",
    "mode_temps_dark_nolight = mode(darknessnolight['Time'])\n",
    "mode_temps_dark_nolight = mode_temps_dark_nolight.strftime('%H:%M')\n",
    "\n",
    "filtre1 = (pd.isnull(dades['Time'])) & (dades['Light_Conditions'] == 'Darkness - no lighting')\n",
    "dades.loc[filtre1, 'Time'] = mode_temps_dark_nolight\n",
    "\n",
    "## daylight\n",
    "daylight = dades[dades['Light_Conditions'] == \"Daylight\"].copy()\n",
    "daylight['Time'] = pd.to_datetime(daylight['Time'], format='%H:%M')\n",
    "mediana_temps_daylight = daylight['Time'].mean()\n",
    "mediana_temps_daylight = mediana_temps_daylight.strftime('%H:%M')\n",
    "\n",
    "filtre2 = (pd.isnull(dades['Time'])) & (dades['Light_Conditions'] == 'Daylight')\n",
    "dades.loc[filtre2, 'Time'] = mediana_temps_daylight\n",
    "\n",
    "## darkness- lights lit\n",
    "darknesslightslit = dades[dades['Light_Conditions'] == \"Darkness - lights lit\"].copy()\n",
    "darknesslightslit['Time'] = pd.to_datetime(darknesslightslit['Time'], format='%H:%M')\n",
    "mode_temps_darknesslightslit = mode(darknesslightslit['Time'])\n",
    "mode_temps_darknesslightslit = mode_temps_darknesslightslit.strftime('%H:%M')\n",
    "\n",
    "filtre3 = (pd.isnull(dades['Time'])) & (dades['Light_Conditions'] == 'Darkness - lights lit')\n",
    "dades.loc[filtre3, 'Time'] = mode_temps_darknesslightslit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bbd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creem nova variable - Moment_Dia\n",
    "dades['Time'] = pd.to_datetime(dades['Time'])\n",
    "\n",
    "filtre1 = (dades['Time'].dt.hour >= 6) & (dades['Time'].dt.hour <= 11) & (dades['Time'].dt.minute <= 59)\n",
    "filtre2 = (dades['Time'].dt.hour >= 12) & (dades['Time'].dt.hour <= 14) & (dades['Time'].dt.minute <= 59)\n",
    "filtre3 = (dades['Time'].dt.hour >= 15) & (dades['Time'].dt.hour <= 19) & (dades['Time'].dt.minute <= 59)\n",
    "filtre4 = (dades['Time'].dt.hour >= 20) & (dades['Time'].dt.hour <= 23) & (dades['Time'].dt.minute <= 59)\n",
    "filtre5 = (dades['Time'].dt.hour >= 0) & (dades['Time'].dt.hour <= 5) & (dades['Time'].dt.minute <= 59)\n",
    "\n",
    "dades.loc[filtre1, 'Moment_Dia'] = 'Matí'\n",
    "dades.loc[filtre2, 'Moment_Dia'] = 'Migdia'\n",
    "dades.loc[filtre3, 'Moment_Dia'] = 'Tarda'\n",
    "dades.loc[filtre4, 'Moment_Dia'] = 'Nit'\n",
    "dades.loc[filtre5, 'Moment_Dia'] = 'Matinada'\n",
    "\n",
    "categorias = ['Matí', 'Migdia', 'Tarda', 'Nit', 'Matinada']\n",
    "moment_dia_dtype = pd.CategoricalDtype(categories=categorias)\n",
    "\n",
    "dades['Moment_Dia'] = dades['Moment_Dia'].astype(moment_dia_dtype)\n",
    "\n",
    "# Eliminem variable Time\n",
    "dades = dades.drop(columns = ['Time','Accident Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carriage_Hazards - Eliminem (3 NA)\n",
    "dades = dades.dropna(subset = ['Carriageway_Hazards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUMMIES\n",
    "# dummies variables amb pocs nivells\n",
    "dades_dummies = dades.drop(['Local_Authority_(District)','Police_Force'],axis=1)\n",
    "dades_dummies = pd.get_dummies(dades_dummies, dummy_na=False)\n",
    "\n",
    "# dummies variable Local Authority District\n",
    "dades_dummies1 = dades.loc[:, [\"Local_Authority_(District)\"]]\n",
    "frequencies_LocalAuthority = dades['Local_Authority_(District)'].value_counts()\n",
    "x = 2500\n",
    "classes_menys_frequents1 =  frequencies_LocalAuthority[frequencies_LocalAuthority < x].index\n",
    "dades_dummies1 = dades_dummies1.replace(classes_menys_frequents1, 'Altres')\n",
    "dades_dummies1 = pd.get_dummies(dades_dummies1)\n",
    "\n",
    "# dummies variable Police Force\n",
    "dades_dummies2 = dades.loc[:, [\"Police_Force\"]]\n",
    "frequencies_PoliceForce = dades['Police_Force'].value_counts()\n",
    "x = 9500\n",
    "classes_menys_frequents2 =  frequencies_PoliceForce[frequencies_PoliceForce < x].index\n",
    "dades_dummies2 = dades_dummies2.replace(classes_menys_frequents2, 'Altres')\n",
    "dades_dummies2 = pd.get_dummies(dades_dummies2)\n",
    "\n",
    "dades_dummies = pd.concat([dades_dummies, dades_dummies1], axis = 1)\n",
    "dades_dummies = pd.concat([dades_dummies, dades_dummies2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformem 0 en NA (variables \"Road_Type\", \"Road_Surface_Conditions\" i \"Weather_Conditions\")\n",
    "selected_columns1 = [col for col in dades_dummies.columns if col.startswith(\"Road_Type\")]\n",
    "\n",
    "for index, row in dades_dummies.iterrows():\n",
    "    if row[selected_columns1].sum() == 0:\n",
    "        dades_dummies.loc[index, selected_columns1] = pd.NA\n",
    "        \n",
    "selected_columns2 = [col for col in dades_dummies.columns if col.startswith(\"Road_Surface_Conditions\")]\n",
    "\n",
    "for index, row in dades_dummies.iterrows():\n",
    "    if row[selected_columns2].sum() == 0:\n",
    "        dades_dummies.loc[index, selected_columns2] = pd.NA\n",
    "        \n",
    "selected_columns3 = [col for col in dades_dummies.columns if col.startswith(\"Weather_Conditions\")]\n",
    "\n",
    "for index, row in dades_dummies.iterrows():\n",
    "    if row[selected_columns3].sum() == 0:\n",
    "        dades_dummies.loc[index, selected_columns3] = pd.NA\n",
    "\n",
    "#dades_dummies.to_csv('dades_dummies2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed34904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputació NA\n",
    "dades_dummies2 = pd.read_csv('dades_dummies2.csv')\n",
    "imputer = KNNImputer(n_neighbors = 5)  # per defecte: strategy = mean\n",
    "dades_imputades = pd.DataFrame(imputer.fit_transform(dades_dummies2), columns = dades_dummies2.columns)\n",
    "\n",
    "#dades_imputades.to_csv('dades_imputades.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df37e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprovem NA\n",
    "dades_imputades = pd.read_csv('dades_imputades.csv')\n",
    "dades_imputades.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd430418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: a les variables dummies on hi hagi el màxim, 0: les altres i en cas d'empat aleatòriament\n",
    "dades_imputades = pd.read_csv('dades_imputades.csv')\n",
    "categories = [\"Road_Type\", \"Road_Surface_Conditions\", \"Weather_Conditions\"]\n",
    "\n",
    "for categoria in categories:\n",
    "    col = dades_imputades.filter(regex=f'^{categoria}', axis=1).columns\n",
    "    \n",
    "    for index, row in dades_imputades.iterrows():\n",
    "        files_val = row[col]\n",
    "        valor_max = files_val.max()\n",
    "        cont_max = (files_val == valor_max).sum()\n",
    "        \n",
    "        if cont_max > 1:\n",
    "            indexs_max = files_val[files_val == valor_max].index.tolist()\n",
    "            index_max = np.random.choice(indexs_max)\n",
    "        else:\n",
    "            max_index = files_val.idxmax()\n",
    "        \n",
    "        dades_imputades.loc[index, col] = 0\n",
    "        dades_imputades.at[index, max_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa423626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconvertim dummies en variables categòriques\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # ignorem warnings\n",
    "\n",
    "\n",
    "patrons_prefixos = {\n",
    "    'Day_of_Week': ('^Day_of_Week_', ''),\n",
    "    'Junction_Control': ('^Junction_Control_', ''),\n",
    "    'Junction_Detail': ('^Junction_Detail_', ''),\n",
    "    'Accident_Severity': ('^Accident_Severity_', ''),\n",
    "    'Light_Conditions': ('^Light_Conditions_', ''),\n",
    "    'Carriageway_Hazards': ('^Carriageway_Hazards_', ''),\n",
    "    'Road_Surface_Conditions': ('^Road_Surface_Conditions_', ''),\n",
    "    'Road_Type': ('^Road_Type_', ''),\n",
    "    'Urban_or_Rural_Area': ('^Urban_or_Rural_Area_', ''),\n",
    "    'Weather_Conditions': ('^Weather_Conditions_', ''),\n",
    "    'Vehicle_Type': ('^Vehicle_Type_', ''),\n",
    "    'Any_Accident': ('^Any_Accident_', ''),\n",
    "    'Moment_Dia': ('^Moment_Dia_', ''),\n",
    "    'Local_Authority_District': ('^Local_Authority_\\(District\\)_', ''),\n",
    "    'Police_Force': ('^Police_Force_', '')\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "dades2 = pd.DataFrame()\n",
    "\n",
    "for i, (patro, prefix) in patrons_prefixos.items():\n",
    "    variables = dades_imputades.filter(regex = patro, axis = 1)\n",
    "    categories = variables.idxmax(axis=1)\n",
    "    categories = categories.str.replace(patro, \"\")\n",
    "\n",
    "    dades2[i] = categories\n",
    "\n",
    "dades2 = pd.concat([dades2, dades_imputades[['Latitude','Longitude','Number_of_Casualties','Number_of_Vehicles',\n",
    "                                       'Speed_limit','Mes_Accident','Dia_Accident']]], axis=1)\n",
    "        \n",
    "#dades2.to_csv('dades2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee59b21",
   "metadata": {},
   "source": [
    "#### 4.2 SYNTHETIC MINORITY OVERSAMPLING TECHNIQUE (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dades2 = pd.read_csv('dades2.csv') \n",
    "\n",
    "# Convertim les variables de tipus object a categòriques\n",
    "for var in dades2.columns:\n",
    "    if dades2[var].dtype == 'object':\n",
    "        dades2[var] = dades2[var].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62ad26",
   "metadata": {},
   "source": [
    "**Tècnica SMOTE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertim les variables categòriques a Dummy (menys la variable resposta)\n",
    "no_dummy = dades2['Accident_Severity']\n",
    "si_dummies = dades2.drop('Accident_Severity', axis=1)  \n",
    "columnes_dummies = pd.get_dummies(si_dummies)\n",
    "dades_dummies2 = pd.concat([no_dummy, columnes_dummies], axis=1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "dades_dummies2['Accident_Severity'] = le.fit_transform(dades2['Accident_Severity']) #0: Fatal, 1: Serious, 2: Slight\n",
    "\n",
    "X = dades_dummies2.drop(['Accident_Severity'], axis=1)\n",
    "y = dades_dummies2['Accident_Severity']\n",
    "\n",
    "smote = SMOTE(random_state=123)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "dades_equilibrades = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "dades_equilibrades['Accident_Severity'] = y_resampled\n",
    "\n",
    "dades_equilibrades.to_csv('dades_equilibrades.csv', index=False)\n",
    "dades_equilibrades = pd.read_csv('dades_equilibrades.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accident_Severity - Dades balancejades\n",
    "frequencia_accident_sev = dades_equilibrades['Accident_Severity'].value_counts().sort_index()\n",
    "colors = ['red','orange','green']\n",
    "start_angles = 90\n",
    "plt.pie(frequencia_accident_sev.values, labels = ['Fatal', 'Serious', 'Slight'], autopct='%1.1f%%',\n",
    "        startangle=start_angles, textprops={'fontsize': 12, 'color': 'white'}, labeldistance=None, colors=colors)\n",
    "plt.title('FREQÜÈNCIA DE LA GRAVETAT DELS ACCIDENTS', fontsize = 15)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1), prop={'size': 12})\n",
    "#plt.savefig('gq_resp_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b2ca3",
   "metadata": {},
   "source": [
    "#### 4.3 SELECCIÓ DE VARIABLES\n",
    "Per continuar amb la selecció de variables he fet dues proves diferents:\n",
    "\n",
    "- Per les variables numèriques: prova ANOVA. Ens retorna les variables numèriques ordenades descendentment segons les seves puntuacions F en ANOVA.\n",
    "- Per les variables categòriques: test chi-quadrat. Ens retorna les variables categòriques més rellevants mitjançant la prova chi-quadrat, ens les retorna ordenades descendentment.\n",
    "- Mitjançant l'algorisme Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e316b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables numèriques (DADES DESBALANCEJADES)\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "X = dades2.drop('Accident_Severity', axis=1) \n",
    "y = dades2['Accident_Severity'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)\n",
    "\n",
    "X_train_numeric = X_train.select_dtypes(include='number')\n",
    "\n",
    "k = 3 \n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "selector.fit(X_train_numeric, y_train)\n",
    "\n",
    "scores = selector.scores_ # prova F\n",
    "variables_scores = pd.DataFrame({'Variable': X_train_numeric.columns, 'Score': scores}) # creem data frame  \n",
    "sorted_variables = variables_scores.sort_values(by='Score', ascending=False) # ordenem descendentment\n",
    "sorted_variables_names = sorted_variables['Variable'].values # obtenim els noms de les variables ordenades descendentment segons F\n",
    "\n",
    "print(sorted_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c893b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables categòriques (DADES DESBALANCEJADES)\n",
    "\n",
    "X_train_categorical = X_train.select_dtypes(include=['object', 'category'])\n",
    "X_train_encoded = pd.get_dummies(X_train_categorical)\n",
    "\n",
    "k = 4  \n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=k)\n",
    "X_train_selected = selector.fit_transform(X_train_encoded, y_train)\n",
    "\n",
    "indexs = selector.get_support(indices=True) # obtenim els índexs de les característiques seleccionades\n",
    "variables = X_train_encoded.columns[indexs] # obtenim els noms de les característiques\n",
    "\n",
    "chi2_scores = selector.scores_ # obtenim valors prova chi-quadrat\n",
    "variables_chi2_scores = pd.DataFrame({'Variable': X_train_encoded.columns, 'Chi2 Score': chi2_scores}) # creem df\n",
    "sorted_variable_chi2_scores = variables_chi2_scores.sort_values(by='Chi2 Score', ascending=False) # ordenem descendentment\n",
    "sorted_variable_names = sorted_variable_chi2_scores['Variable'].values # ordenem els noms \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(variables_chi2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e21bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtre = [\"Day_of_Week_\",\"Light_Conditions_\",\"Carriageway_Hazards_\",\"Road_Surface_Conditions_\",\n",
    "          \"Road_Type_\",\"Urban_or_Rural_Area_\",\"Weather_Conditions_\",\"Moment_Dia_\",\"Police_Force_\",\n",
    "                              \"Junction_Detail_\",\"Junction_Control_\",\"Local_Authority_District_\",\n",
    "                              \"Vehicle_Type_\"]\n",
    "\n",
    "chi_quadrat_resultats = {}\n",
    "\n",
    "for i in filtre:\n",
    "    filtered_rows = variables_chi2_scores[variables_chi2_scores['Variable'].str.startswith(i)]\n",
    "    valors = filtered_rows['Chi2 Score'].tolist()\n",
    "    \n",
    "    if valors:\n",
    "        chi_quadrat_resultats[i] = sum(valors) / len(valors)\n",
    "    else:\n",
    "        chi_quadrat_resultats[i] = 0\n",
    "\n",
    "\n",
    "print(\"Mitjana del valor Chi2 Score per variable:\")\n",
    "for grup, mean in chi_quadrat_resultats.items():\n",
    "    print(f\"'{grup}': {mean}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b797880e",
   "metadata": {},
   "source": [
    "Mateix procediment però amb les **dades balancejades**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdadbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES NUMÈRIQUES\n",
    "## reconvertim dummies en variables categòriques\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # ignorem warnings\n",
    "patrons_prefixos = {\n",
    "    'Day_of_Week': ('^Day_of_Week_', ''),\n",
    "    'Junction_Control': ('^Junction_Control_', ''),\n",
    "    'Junction_Detail': ('^Junction_Detail_', ''),\n",
    "    'Light_Conditions': ('^Light_Conditions_', ''),\n",
    "    'Carriageway_Hazards': ('^Carriageway_Hazards_', ''),\n",
    "    'Road_Surface_Conditions': ('^Road_Surface_Conditions_', ''),\n",
    "    'Road_Type': ('^Road_Type_', ''),\n",
    "    'Urban_or_Rural_Area': ('^Urban_or_Rural_Area_', ''),\n",
    "    'Vehicle_Type': ('^Vehicle_Type_', ''),\n",
    "    'Weather_Conditions': ('^Weather_Conditions_', ''),\n",
    "    'Moment_Dia': ('^Moment_Dia_', ''),\n",
    "    'Local_Authority_District': ('^Local_Authority_District_', ''),\n",
    "    'Police_Force': ('^Police_Force_', '')\n",
    "}\n",
    "\n",
    "df_eq = pd.DataFrame()\n",
    "\n",
    "for i, (patro, prefix) in patrons_prefixos.items():\n",
    "    variables = dades_equilibrades.filter(regex = patro, axis = 1)\n",
    "    categories = variables.idxmax(axis=1)\n",
    "    categories = categories.str.replace(patro, \"\")\n",
    "\n",
    "    df_eq[i] = categories\n",
    "\n",
    "dades_equilibrades2 = pd.concat([df_eq, dades_equilibrades[['Latitude','Longitude','Number_of_Casualties',\n",
    "                                                            'Number_of_Vehicles','Speed_limit','Mes_Accident',\n",
    "                                                            'Any_Accident','Dia_Accident','Accident_Severity']]], axis=1)\n",
    "        \n",
    "\n",
    "X = dades_equilibrades2.drop('Accident_Severity', axis=1) \n",
    "y = dades_equilibrades2['Accident_Severity'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\n",
    "\n",
    "X_train_numeric = X_train.select_dtypes(include='number')\n",
    "\n",
    "k = 3 \n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "selector.fit(X_train_numeric, y_train)\n",
    "\n",
    "scores = selector.scores_ # prova F\n",
    "variables_scores = pd.DataFrame({'Variable': X_train_numeric.columns, 'Score': scores}) \n",
    "sorted_variables = variables_scores.sort_values(by='Score', ascending=False) \n",
    "sorted_variables_names = sorted_variables['Variable'].values # variables ordenades descendentment segons F\n",
    "\n",
    "print(sorted_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf7ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES CATEGÒRIQUES\n",
    "X_train_categorical = X_train.select_dtypes(include=['object', 'category'])\n",
    "X_train_encoded = pd.get_dummies(X_train_categorical)\n",
    "\n",
    "k = 4  \n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=k)\n",
    "X_train_selected = selector.fit_transform(X_train_encoded, y_train)\n",
    "\n",
    "indexs = selector.get_support(indices=True) # obtenim indexs característiques seleccionades\n",
    "variables = X_train_encoded.columns[indexs] # obtenim noms de les característiques\n",
    "\n",
    "chi2_scores = selector.scores_ # obtenim valors prova chi-quadrat\n",
    "variables_chi2_scores = pd.DataFrame({'Variable': X_train_encoded.columns, 'Chi2 Score': chi2_scores}) # creem df\n",
    "sorted_variable_chi2_scores = variables_chi2_scores.sort_values(by='Chi2 Score', ascending=False) # ordenem descendentment\n",
    "sorted_variable_names = sorted_variable_chi2_scores['Variable'].values # ordenem els noms \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "filtre = [\"Day_of_Week_\",\"Light_Conditions_\",\"Carriageway_Hazards_\",\"Road_Surface_Conditions_\",\n",
    "          \"Road_Type_\",\"Urban_or_Rural_Area_\",\"Weather_Conditions_\",\"Moment_Dia_\",\"Police_Force_\",\n",
    "                              \"Junction_Detail_\",\"Junction_Control_\",\"Local_Authority_District_\",\n",
    "                              \"Vehicle_Type_\"]\n",
    "\n",
    "chi_quadrat_resultats = {}\n",
    "\n",
    "for i in filtre:\n",
    "    filtered_rows = variables_chi2_scores[variables_chi2_scores['Variable'].str.startswith(i)]\n",
    "    valors = filtered_rows['Chi2 Score'].tolist()\n",
    "    \n",
    "    if valors:\n",
    "        chi_quadrat_resultats[i] = sum(valors) / len(valors)\n",
    "    else:\n",
    "        chi_quadrat_resultats[i] = 0\n",
    "\n",
    "chi_quadrat_resultats = dict(sorted(chi_quadrat_resultats.items(), key = lambda item: item [1], reverse = True))\n",
    "\n",
    "print(\"Mitjana del valor Chi2 Score per variable:\")\n",
    "for grup, mean in chi_quadrat_resultats.items():\n",
    "    print(f\"'{grup}': {mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ec1a8",
   "metadata": {},
   "source": [
    "Importància de les variables mitjançant Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f10a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busquem els millors paràmetres per realitzar un Random Forest\n",
    "X = dades_equilibrades.drop('Accident_Severity', axis=1) \n",
    "y = dades_equilibrades['Accident_Severity'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 3 ,5, 7, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 20, 50],\n",
    "    'min_samples_split': [2, 5, 10, 20, 50, 100],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': [\"gini\", \"entropy\", \"log_loss\"]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 123)\n",
    "\n",
    "search = HalvingGridSearchCV(rf, param_grid, cv=5, factor=2)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(search.best_params_)\n",
    "print(np.mean(search.best_score_))\n",
    "\n",
    "#bootstrap: False, criterion: 'entropy', max_depth: None, min_samples_leaf: 1, min_samples_split: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c533e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirem la importància de cada una de les variables entrenant un Random Forest\n",
    "X = dades_equilibrades.drop('Accident_Severity', axis=1) \n",
    "y = dades_equilibrades['Accident_Severity'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 123)\n",
    "\n",
    "parametres = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'bootstrap': [False],\n",
    "    'criterion': ['entropy']\n",
    "}\n",
    "\n",
    "model_rf = GridSearchCV(estimator = rf, param_grid = parametres, cv = 5, scoring = 'accuracy', n_jobs = -1)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "millors_parametres = model_rf.best_params_\n",
    "millor_model = model_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4910aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "importancies = millor_model.feature_importances_\n",
    "\n",
    "df_importancies = pd.DataFrame({\n",
    "    'Variable': X.columns,\n",
    "    'Importància': importancies\n",
    "})\n",
    "\n",
    "df_importancies = df_importancies.sort_values(by='Importància', ascending=False)\n",
    "\n",
    "filtre = [\"Any_Accident\", \"Number_of_Casualties\", \"Number_of_Vehicles\", \"Speed_limit\", \"Mes_Accident\",\n",
    "          \"Dia_Accident\", \"Day_of_Week_\", \"Junction_Control_\", \"Junction_Detail_\", \"Light_Conditions_\",\"Carriageway_Hazards_\",\n",
    "          \"Road_Surface_Conditions_\", \"Road_Type_\",\"Urban_or_Rural_Area_\",\"Weather_Conditions_\", \"Vehicle_Type_\", \n",
    "          \"Moment_Dia_\",\"Police_Force_\",\"Local_Authority_District_\",\"Latitude\",\"Longitude\"]\n",
    "\n",
    "mitjanes_importancies = {}\n",
    "\n",
    "for i in filtre:\n",
    "    df_prefixos = df_importancies[df_importancies['Variable'].str.startswith(i)]\n",
    "    mitjana = df_prefixos['Importància'].mean()\n",
    "    mitjanes_importancies[i] = mitjana\n",
    "\n",
    "mitjanes_importancies = pd.DataFrame(list(mitjanes_importancies.items()), columns=['Variable', 'Importància'])\n",
    "#mitjanes_importancies.to_csv('mitjanes_importancies_tot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mitjanes_importancies = pd.read_csv('mitjanes_importancies_tot.csv') \n",
    "mitjanes_importancies.sort_values(by = \"Importància\", ascending = False)\n",
    "\n",
    "dades2 = dades2.drop(columns=['Carriageway_Hazards','Local_Authority_District', 'Vehicle_Type', 'Weather_Conditions',\n",
    "                              'Junction_Detail', 'Police_Force', 'Road_Type', 'Road_Surface_Conditions'])\n",
    "\n",
    "var_eliminar = ['Carriageway_Hazards','Local_Authority_District', 'Vehicle_Type', 'Weather_Conditions',\n",
    "                              'Junction_Detail', 'Police_Force', 'Road_Type', 'Road_Surface_Conditions']\n",
    "\n",
    "eliminem = [x for x in dades_equilibrades.keys()\n",
    "                     if any(x.startswith(var) for var in var_eliminar)]\n",
    "for x in eliminem:\n",
    "    del dades_equilibrades[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1671169",
   "metadata": {},
   "source": [
    "### 5. MÈTRIQUES PER AVALUAR LA CAPACITAT PREDICTIVA\n",
    "### 6.MODELS PREDICTIUS    \n",
    "#### 6.1 REGRESSIÓ LOGÍSTICA MULTINOMIAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f636eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dades inicials (desbalancejades)\n",
    "print(dades2['Accident_Severity'].value_counts())\n",
    "\n",
    "# Tècnica smote aplicada (balancejades)\n",
    "print(dades_equilibrades['Accident_Severity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfecce6",
   "metadata": {},
   "source": [
    "**Base de dades inicial (desequilibri de classes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbec454",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dummy = dades2['Accident_Severity']\n",
    "si_dummies = dades2.drop('Accident_Severity', axis=1)  \n",
    "columnes_dummies = pd.get_dummies(si_dummies)\n",
    "dades_desequilibrades = pd.concat([no_dummy, columnes_dummies], axis=1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "dades_desequilibrades['Accident_Severity'] = le.fit_transform(dades_desequilibrades['Accident_Severity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d368dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X = dades_desequilibrades.drop('Accident_Severity', axis=1)\n",
    "y = dades_desequilibrades['Accident_Severity']\n",
    "\n",
    "escalar = StandardScaler()\n",
    "X_escalat = pd.DataFrame(escalar.fit_transform(X))\n",
    "\n",
    "noms_classes = [\"Fatal\", \"Serious\", \"Slight\"] # 0, 1, 2\n",
    "\n",
    "model_reglog = LogisticRegression(\n",
    "    multi_class = 'multinomial',\n",
    "    solver = 'lbfgs',\n",
    "    max_iter = 2000\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 123)\n",
    "\n",
    "accuracy_reglog_0 =[]\n",
    "precisio_reglog_0_3, precisio_reglog_0 = [], []\n",
    "f1_reglog_0_3, f1_reglog_0 = [], []\n",
    "sensibilitat_reglog_0_3, sensibilitat_reglog_0 = [], []\n",
    "balanced_accuracy_reglog_0 = []\n",
    "mc_reglog_0 = []\n",
    "especificitat_reglog_0_3, especificitat_reglog_0 = [], []\n",
    "mae_reglog_0 = []\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    X_train, X_test = X_escalat.iloc[train_index], X_escalat.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "   \n",
    "    model_reglog.fit(X_train, y_train)\n",
    "    y_pred_reglog = model_reglog.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred_reglog)\n",
    "    cm_df = pd.DataFrame(cm, index = noms_classes, columns = noms_classes)\n",
    "   \n",
    "    # Mètriques\n",
    "    # Accuracy, Precisió, F1 Score, Sensibilitat\n",
    "    accuracy_reglog_0.append(accuracy_score(y_test, y_pred_reglog))\n",
    "    precisio_reglog_0_3.append(precision_score(y_test, y_pred_reglog, average=None))\n",
    "    precisio_reglog_0.append(precision_score(y_test, y_pred_reglog, average = 'weighted'))\n",
    "    f1_reglog_0_3.append(f1_score(y_test, y_pred_reglog, average = None))\n",
    "    f1_reglog_0.append(f1_score(y_test, y_pred_reglog, average = 'weighted'))\n",
    "    sensibilitat_reglog_0_3.append(recall_score(y_test, y_pred_reglog, average = None))\n",
    "    sensibilitat_ponderada = recall_score(y_test, y_pred_reglog, average = 'weighted')\n",
    "    sensibilitat_reglog_0.append(sensibilitat_ponderada)\n",
    "    mc_reglog_0.append(cm)\n",
    "   \n",
    "    # Especificitat\n",
    "    x = np.diag(cm)\n",
    "\n",
    "    for i in noms_classes:\n",
    "        index = noms_classes.index(i)\n",
    "        TN = cm_df.drop(i, axis = 0).drop(i, axis = 1).values.sum()\n",
    "        FP = cm_df[i].sum() - cm_df.loc[i, i]\n",
    "        especificitat = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        especificitat_reglog_0_3.append(especificitat)\n",
    "       \n",
    "    especificitat_ponderada = (especificitat_reglog_0_3[0]*x[0] + especificitat_reglog_0_3[1]*x[1] +\n",
    "                        especificitat_reglog_0_3[2]*x[2]) / sum(x)\n",
    "   \n",
    "    especificitat_reglog_0.append(especificitat_ponderada)\n",
    "   \n",
    "    # Balanced Accuracy\n",
    "    ba = (sensibilitat_ponderada + especificitat_ponderada) / 2\n",
    "    balanced_accuracy_reglog_0.append(ba)\n",
    "   \n",
    "    # MAE\n",
    "    pesos = np.array([\n",
    "    [0, 1, 2],\n",
    "    [1, 0, 1],\n",
    "    [2, 1, 0]\n",
    "    ])\n",
    "\n",
    "    n = np.sum(cm)\n",
    "    mae = np.sum(cm * pesos) / n\n",
    "    mae_reglog_0.append(mae)\n",
    "   \n",
    "end_time = time.time()\n",
    "total_time_reglog_0 = round(end_time - start_time,2)\n",
    "   \n",
    "# Matriu de confusió\n",
    "matriu_conf_reglog_0 = np.round(np.mean(mc_reglog_0, axis = 0))\n",
    "matriu_conf_reglog_0_df = pd.DataFrame(matriu_conf_reglog_0, index=[\"Fatal\", \"Serious\", \"Slight\"],\n",
    "                                   columns=[\"Fatal\", \"Serious\", \"Slight\"])\n",
    "matriu_conf_reglog_0_df = np.round(matriu_conf_reglog_0_df)\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "np.set_printoptions(formatter={'float': '{:.0f}'.format})\n",
    "\n",
    "sns.heatmap(matriu_conf_reglog_0_df, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Classe predita', fontsize = 15)\n",
    "plt.ylabel('Classe vertadera', fontsize = 15)\n",
    "plt.xticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.yticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.title('Matriu de Confusió', fontsize = 17)\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mètriques\n",
    "np.set_printoptions(precision=3)\n",
    "print(especificitat_reglog_0_3)\n",
    "print(especificitat_reglog_0)\n",
    "print(mae_reglog_0)\n",
    "print(f1_reglog_0_3)\n",
    "print(precisio_reglog_0_3)\n",
    "print(balanced_accuracy_reglog_0)\n",
    "print(sensibilitat_reglog_0)\n",
    "print(f1_reglog_0)\n",
    "print(accuracy_reglog_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd6f224",
   "metadata": {},
   "source": [
    "**Base de dades (classes balancejades - SMOTE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7dc2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X = dades_equilibrades.drop('Accident_Severity', axis=1)\n",
    "y = dades_equilibrades['Accident_Severity']\n",
    "\n",
    "escalar = StandardScaler()\n",
    "X_escalat = pd.DataFrame(escalar.fit_transform(X))\n",
    "\n",
    "noms_classes = [\"Fatal\", \"Serious\", \"Slight\"] # 0, 1, 2\n",
    "\n",
    "model_reglog = LogisticRegression(\n",
    "    multi_class = 'multinomial',\n",
    "    solver = 'lbfgs',\n",
    "    max_iter = 2000\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 123)\n",
    "\n",
    "accuracy_reglog =[]\n",
    "precisio_reglog_3, precisio_reglog = [], []\n",
    "f1_reglog_3, f1_reglog = [], []\n",
    "sensibilitat_reglog_3, sensibilitat_reglog = [], []\n",
    "balanced_accuracy_reglog = []\n",
    "mc_reglog = []\n",
    "especificitat_reglog_3, especificitat_reglog = [], []\n",
    "mae_reglog = []\n",
    "\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    X_train, X_test = X_escalat.iloc[train_index], X_escalat.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "   \n",
    "    model_reglog.fit(X_train, y_train)\n",
    "    y_pred_reglog = model_reglog.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred_reglog)\n",
    "    cm_df = pd.DataFrame(cm, index = noms_classes, columns = noms_classes)\n",
    "   \n",
    "    # Mètriques\n",
    "    # Accuracy, Precisió, F1 Score, Sensibilitat\n",
    "    accuracy_reglog.append(accuracy_score(y_test, y_pred_reglog))\n",
    "    precisio_reglog_3.append(precision_score(y_test, y_pred_reglog, average=None))\n",
    "    precisio_reglog.append(precision_score(y_test, y_pred_reglog, average = 'weighted'))\n",
    "    f1_reglog_3.append(f1_score(y_test, y_pred_reglog, average = None))\n",
    "    f1_reglog.append(f1_score(y_test, y_pred_reglog, average = 'weighted'))\n",
    "    sensibilitat_reglog_3.append(recall_score(y_test, y_pred_reglog, average = None))\n",
    "    sensibilitat_ponderada = recall_score(y_test, y_pred_reglog, average = 'weighted')\n",
    "    sensibilitat_reglog.append(sensibilitat_ponderada)\n",
    "    mc_reglog.append(cm)\n",
    "   \n",
    "    # Especificitat\n",
    "    x = np.diag(cm)\n",
    "\n",
    "    for i in noms_classes:\n",
    "        index = noms_classes.index(i)\n",
    "        TN = cm_df.drop(i, axis = 0).drop(i, axis = 1).values.sum()\n",
    "        FP = cm_df[i].sum() - cm_df.loc[i, i]\n",
    "        especificitat = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        especificitat_reglog_3.append(especificitat)\n",
    "       \n",
    "    especificitat_ponderada = (especificitat_reglog_3[0]*x[0] + especificitat_reglog_3[1]*x[1] +\n",
    "                        especificitat_reglog_3[2]*x[2]) / sum(x)\n",
    "   \n",
    "    especificitat_reglog.append(especificitat_ponderada)\n",
    "   \n",
    "    # Balanced Accuracy\n",
    "    ba = (sensibilitat_ponderada + especificitat_ponderada) / 2\n",
    "    balanced_accuracy_reglog.append(ba)\n",
    "   \n",
    "    # MAE\n",
    "    pesos = np.array([\n",
    "    [0, 1, 2],\n",
    "    [1, 0, 1],\n",
    "    [2, 1, 0]\n",
    "    ])\n",
    "\n",
    "    n = np.sum(cm)\n",
    "    mae = np.sum(cm * pesos) / n\n",
    "    mae_reglog.append(mae)\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time_reglog = round(end_time - start_time,2)\n",
    "   \n",
    "# Matriu de confusió\n",
    "matriu_conf_reglog = np.round(np.mean(mc_reglog, axis = 0))\n",
    "matriu_conf_reglog_df = pd.DataFrame(matriu_conf_reglog, index=[\"Fatal\", \"Serious\", \"Slight\"],\n",
    "                                   columns=[\"Fatal\", \"Serious\", \"Slight\"])\n",
    "matriu_conf_reglog_df = np.round(matriu_conf_reglog_df)\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "np.set_printoptions(formatter={'float': '{:.0f}'.format})\n",
    "\n",
    "sns.heatmap(matriu_conf_reglog_df, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Classe predita', fontsize = 15)\n",
    "plt.ylabel('Classe vertadera', fontsize = 15)\n",
    "plt.xticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.yticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.title('Matriu de Confusió', fontsize = 17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe21737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mètriques\n",
    "np.set_printoptions(precision=4)\n",
    "print(especificitat_reglog_3)\n",
    "print(especificitat_reglog)\n",
    "print(mae_reglog)\n",
    "print(f1_reglog_3)\n",
    "print(precisio_reglog_3)\n",
    "print(balanced_accuracy_reglog)\n",
    "print(sensibilitat_reglog)\n",
    "print(f1_reglog)\n",
    "print(accuracy_reglog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01fdcd8",
   "metadata": {},
   "source": [
    "#### 6.2 RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busquem els millors paràmetres per realitzar un Random Forest\n",
    "X = dades_equilibrades.drop('Accident_Severity', axis=1) \n",
    "y = dades_equilibrades['Accident_Severity'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 3 ,5, 7, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 5, 10, 20, 50],\n",
    "    'min_samples_split': [2, 5, 10, 20, 50, 100],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': [\"gini\", \"entropy\", \"log_loss\"]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 123)\n",
    "\n",
    "search = HalvingGridSearchCV(rf, param_grid, cv = 5, factor = 2,  n_jobs = -1)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(search.best_params_)\n",
    "print(np.mean(search.best_score_))\n",
    "#{'bootstrap': False, 'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2facc2d",
   "metadata": {},
   "source": [
    "**Base de dades inicial (desequilibri de classes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8639a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X = dades_desequilibrades.drop('Accident_Severity', axis=1)\n",
    "y = dades_desequilibrades['Accident_Severity']\n",
    "\n",
    "noms_classes = [\"Fatal\", \"Serious\", \"Slight\"]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_rf = RandomForestClassifier(\n",
    "    n_estimators=300,        \n",
    "    max_depth=None,          \n",
    "    min_samples_leaf=1,      \n",
    "    min_samples_split=5,\n",
    "    bootstrap=False,\n",
    "    criterion='gini',\n",
    "    random_state=123          \n",
    ")\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "accuracy_rf_0 = []\n",
    "precisio_rf_0_3 = []\n",
    "precisio_rf_0 = []\n",
    "f1_rf_0_3 = []\n",
    "f1_rf_0 = []\n",
    "sensibilitat_rf_0_3 = []\n",
    "sensibilitat_rf_0 = []\n",
    "balanced_accuracy_rf_0 = []\n",
    "mc_rf_0 = []\n",
    "especificitat_rf_0_3 = []\n",
    "especificitat_rf_0 = []\n",
    "mae_rf_0 = []\n",
    "\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    y_pred_rf = model_rf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred_rf)\n",
    "    cm_df = pd.DataFrame(cm, index = noms_classes, columns = noms_classes)\n",
    "\n",
    "    # Mètriques\n",
    "    # Accuracy, Precisió, F1 Score, Sensibilitat\n",
    "    accuracy_rf_0.append(accuracy_score(y_test, y_pred_rf))\n",
    "    precisio_rf_0_3.append(precision_score(y_test, y_pred_rf, average=None))\n",
    "    precisio_rf_0.append(precision_score(y_test, y_pred_rf, average = 'weighted'))\n",
    "    f1_rf_0_3.append(f1_score(y_test, y_pred_rf, average = None))\n",
    "    f1_rf_0.append(f1_score(y_test, y_pred_rf, average = 'weighted'))\n",
    "    sensibilitat_rf_0_3.append(recall_score(y_test, y_pred_rf, average = None))\n",
    "    sensibilitat_ponderada = recall_score(y_test, y_pred_rf, average = 'weighted')\n",
    "    sensibilitat_rf_0.append(sensibilitat_ponderada)\n",
    "    mc_rf_0.append(cm)\n",
    "   \n",
    "    # Especificitat\n",
    "    x = np.diag(cm)\n",
    "\n",
    "    for i in noms_classes:\n",
    "        index = noms_classes.index(i)\n",
    "        TN = cm_df.drop(i, axis = 0).drop(i, axis = 1).values.sum()\n",
    "        FP = cm_df[i].sum() - cm_df.loc[i, i]\n",
    "        especificitat = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        especificitat_rf_0_3.append(especificitat)\n",
    "       \n",
    "    especificitat_ponderada = (especificitat_rf_0_3[0]*x[0] + especificitat_rf_0_3[1]*x[1] +\n",
    "                        especificitat_rf_0_3[2]*x[2]) / sum(x)\n",
    "   \n",
    "    especificitat_rf_0.append(especificitat_ponderada)\n",
    "\n",
    "\n",
    "    # Balanced Accuracy\n",
    "    ba = (sensibilitat_ponderada + especificitat_ponderada) / 2\n",
    "    balanced_accuracy_rf_0.append(ba)\n",
    "   \n",
    "    # MAE\n",
    "    pesos = np.array([\n",
    "    [0, 1, 2],\n",
    "    [1, 0, 1],\n",
    "    [2, 1, 0]\n",
    "    ])\n",
    "\n",
    "    n = np.sum(cm)\n",
    "    mae = np.sum(cm * pesos) / n\n",
    "    mae_rf_0.append(mae)\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time_rf_0 = round(end_time - start_time,2)\n",
    "\n",
    "# Matriu de confusió\n",
    "matriu_conf_rf_0 = np.round(np.mean(mc_rf_0, axis = 0))\n",
    "matriu_conf_rf_0_df = pd.DataFrame(matriu_conf_rf_0, index=[\"Fatal\", \"Serious\", \"Slight\"],\n",
    "                                   columns=[\"Fatal\", \"Serious\", \"Slight\"])\n",
    "matriu_conf_rf_0_df = np.round(matriu_conf_rf_0_df)\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "np.set_printoptions(formatter={'float': '{:.0f}'.format})\n",
    "\n",
    "sns.heatmap(matriu_conf_rf_0_df, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Classe predita', fontsize = 15)\n",
    "plt.ylabel('Classe vertadera', fontsize = 15)\n",
    "plt.xticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.yticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.title('Matriu de Confusió', fontsize = 17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f2e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mètriques\n",
    "np.set_printoptions(precision=2)\n",
    "print(especificitat_rf_0_3)\n",
    "print(especificitat_rf_0)\n",
    "print(mae_rf_0)\n",
    "print(f1_rf_0_3)\n",
    "print(precisio_rf_0_3)\n",
    "print(balanced_accuracy_rf_0)\n",
    "print(sensibilitat_rf_0)\n",
    "print(accuracy_rf_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9219225",
   "metadata": {},
   "source": [
    "**Base de dades (classes balancejades - SMOTE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X = dades_equilibrades.drop('Accident_Severity', axis=1)\n",
    "y = dades_equilibrades['Accident_Severity']\n",
    "\n",
    "noms_classes = [\"Fatal\", \"Serious\", \"Slight\"] # 0, 1, 2\n",
    "\n",
    "model_rf = RandomForestClassifier(\n",
    "    n_estimators=300,        \n",
    "    max_depth=None,          \n",
    "    min_samples_leaf=1,      \n",
    "    min_samples_split=5,\n",
    "    bootstrap=False,\n",
    "    criterion='gini',\n",
    "    random_state=123            \n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "accuracy_rf = []\n",
    "precisio_rf_3 = []\n",
    "precisio_rf = []\n",
    "f1_rf_3 = []\n",
    "f1_rf = []\n",
    "sensibilitat_rf_3 = []\n",
    "sensibilitat_rf = []\n",
    "balanced_accuracy_rf = []\n",
    "mc_rf = []\n",
    "especificitat_rf_3 = []\n",
    "especificitat_rf = []\n",
    "mae_rf = []\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    y_pred_rf = model_rf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred_rf)\n",
    "    cm_df = pd.DataFrame(cm, index = noms_classes, columns = noms_classes) # diferent cada vegada\n",
    "\n",
    "    # Mètriques\n",
    "    # Accuracy, Precisió, F1 Score, Sensibilitat\n",
    "    accuracy_rf.append(accuracy_score(y_test, y_pred_rf))\n",
    "    precisio_rf_3.append(precision_score(y_test, y_pred_rf, average=None))\n",
    "    precisio_rf.append(precision_score(y_test, y_pred_rf, average = 'weighted'))\n",
    "    f1_rf_3.append(f1_score(y_test, y_pred_rf, average = None))\n",
    "    f1_rf.append(f1_score(y_test, y_pred_rf, average = 'weighted'))\n",
    "    sensibilitat_rf_3.append(recall_score(y_test, y_pred_rf, average = None))\n",
    "    sensibilitat_ponderada = recall_score(y_test, y_pred_rf, average = 'weighted')\n",
    "    sensibilitat_rf.append(sensibilitat_ponderada)\n",
    "    mc_rf.append(cm) # anem guardant totes les matrius de confusió\n",
    "   \n",
    "    # Especificitat\n",
    "    x = np.diag(cm)\n",
    "\n",
    "    for i in noms_classes:\n",
    "        index = noms_classes.index(i)\n",
    "        TN = cm_df.drop(i, axis = 0).drop(i, axis = 1).values.sum()\n",
    "        FP = cm_df[i].sum() - cm_df.loc[i, i]\n",
    "        especificitat = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        especificitat_rf_3.append(especificitat)\n",
    "       \n",
    "    especificitat_ponderada = (especificitat_rf_3[0]*x[0] + especificitat_rf_3[1]*x[1] +\n",
    "                        especificitat_rf_3[2]*x[2]) / sum(x)\n",
    "   \n",
    "    especificitat_rf.append(especificitat_ponderada)\n",
    "\n",
    "\n",
    "    # Balanced Accuracy\n",
    "    ba = (sensibilitat_ponderada + especificitat_ponderada) / 2\n",
    "    balanced_accuracy_rf.append(ba)\n",
    "   \n",
    "    # MAE\n",
    "    pesos = np.array([\n",
    "    [0, 1, 2],\n",
    "    [1, 0, 1],\n",
    "    [2, 1, 0]\n",
    "    ])\n",
    "\n",
    "    n = np.sum(cm)\n",
    "    mae = np.sum(cm * pesos) / n\n",
    "    mae_rf.append(mae)\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time_rf = round(end_time - start_time,2)\n",
    "\n",
    "# Matriu de confusió\n",
    "matriu_conf_rf = np.round(np.mean(mc_rf, axis = 0))\n",
    "matriu_conf_rf_df = pd.DataFrame(matriu_conf_rf, index=[\"Fatal\", \"Serious\", \"Slight\"],\n",
    "                                   columns=[\"Fatal\", \"Serious\", \"Slight\"])\n",
    "matriu_conf_rf_df = np.round(matriu_conf_rf_df)\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "np.set_printoptions(formatter={'float': '{:.0f}'.format})\n",
    "\n",
    "sns.heatmap(matriu_conf_rf_df, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Classe predita', fontsize = 15)\n",
    "plt.ylabel('Classe vertadera', fontsize = 15)\n",
    "plt.xticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.yticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.title('Matriu de Confusió', fontsize = 17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb155d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mètriques\n",
    "np.set_printoptions(precision=2)\n",
    "print(especificitat_rf_3)\n",
    "print(especificitat_rf)\n",
    "print(mae_rf)\n",
    "print(f1_rf_3)\n",
    "print(precisio_rf_3)\n",
    "print(balanced_accuracy_rf)\n",
    "print(sensibilitat_rf)\n",
    "print(accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a06f1",
   "metadata": {},
   "source": [
    "#### 6.3 GRADIENT BOOSTING (XGBoost)\n",
    "**Base de dades inicial (desequilibri de classes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busquem els millors paràmetres per realitzar un XGBoost\n",
    "X = dades_equilibrades.drop('Accident_Severity', axis=1) \n",
    "y = dades_equilibrades['Accident_Severity'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 3 ,5, 7, 10, 15],\n",
    "    'learning_rate': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3],\n",
    "    'subsample': [0.25, 0.5, 1],\n",
    "    'colsample_bytree': [0.2, 0.4, 0.6, 0.8, 1],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'gamma': [0, 0.3, 0.6, 1]\n",
    "    \n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(random_state = 123, objective = 'multi:softmax', n_estimators = 300)\n",
    "\n",
    "search = HalvingGridSearchCV(xgb, param_grid, cv = 5, factor = 2,  n_jobs = -1)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(search.best_params_)\n",
    "print(np.mean(search.best_score_))\n",
    "#{'colsample_bytree': 0.8, 'gamma': 0.6, 'learning_rate': 0.05, 'max_depth': 15, 'subsample': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X = dades_desequilibrades.drop('Accident_Severity', axis=1)\n",
    "y = dades_desequilibrades['Accident_Severity']\n",
    "\n",
    "noms_classes = [\"Fatal\", \"Serious\", \"Slight\"]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_xgb = XGBClassifier(\n",
    "    objective = 'multi:softmax',\n",
    "    n_estimators = 300,\n",
    "    max_depth = 15,\n",
    "    learning_rate = 0.05,\n",
    "    subsample = 0.5,\n",
    "    colsample_bytree = 0.8,\n",
    "    random_state = 123)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "accuracy_xgb_0 = []\n",
    "precisio_xgb_0_3 = []\n",
    "precisio_xgb_0 = []\n",
    "f1_xgb_0_3 = []\n",
    "f1_xgb_0 = []\n",
    "sensibilitat_xgb_0_3 = []\n",
    "sensibilitat_xgb_0 = []\n",
    "balanced_accuracy_xgb_0 = []\n",
    "mc_xgb_0 = []\n",
    "especificitat_xgb_0_3 = []\n",
    "especificitat_xgb_0 = []\n",
    "mae_xgb_0 = []\n",
    "\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    y_pred_xgb = model_xgb.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "    cm_df = pd.DataFrame(cm, index = noms_classes, columns = noms_classes)\n",
    "\n",
    "    # Mètriques\n",
    "    # Accuracy, Precisió, F1 Score, Sensibilitat\n",
    "    accuracy_xgb_0.append(accuracy_score(y_test, y_pred_xgb))\n",
    "    precisio_xgb_0_3.append(precision_score(y_test, y_pred_xgb, average=None))\n",
    "    precisio_xgb_0.append(precision_score(y_test, y_pred_xgb, average = 'weighted'))\n",
    "    f1_xgb_0_3.append(f1_score(y_test, y_pred_xgb, average = None))\n",
    "    f1_xgb_0.append(f1_score(y_test, y_pred_xgb, average = 'weighted'))\n",
    "    sensibilitat_xgb_0_3.append(recall_score(y_test, y_pred_xgb, average = None))\n",
    "    sensibilitat_ponderada = recall_score(y_test, y_pred_xgb, average = 'weighted')\n",
    "    sensibilitat_xgb_0.append(sensibilitat_ponderada)\n",
    "    mc_xgb_0.append(cm)\n",
    "   \n",
    "    # Especificitat\n",
    "    x = np.diag(cm)\n",
    "\n",
    "    for i in noms_classes:\n",
    "        index = noms_classes.index(i)\n",
    "        TN = cm_df.drop(i, axis = 0).drop(i, axis = 1).values.sum()\n",
    "        FP = cm_df[i].sum() - cm_df.loc[i, i]\n",
    "        especificitat = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        especificitat_xgb_0_3.append(especificitat)\n",
    "       \n",
    "    especificitat_ponderada = (especificitat_xgb_0_3[0]*x[0] + especificitat_xgb_0_3[1]*x[1] +\n",
    "                        especificitat_xgb_0_3[2]*x[2]) / sum(x)\n",
    "   \n",
    "    especificitat_xgb_0.append(especificitat_ponderada)\n",
    "\n",
    "\n",
    "    # Balanced Accuracy\n",
    "    ba = (sensibilitat_ponderada + especificitat_ponderada) / 2\n",
    "    balanced_accuracy_xgb_0.append(ba)\n",
    "   \n",
    "    # MAE\n",
    "    pesos = np.array([\n",
    "    [0, 1, 2],\n",
    "    [1, 0, 1],\n",
    "    [2, 1, 0]\n",
    "    ])\n",
    "\n",
    "    n = np.sum(cm)\n",
    "    mae = np.sum(cm * pesos) / n\n",
    "    mae_xgb_0.append(mae)\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time_xgb_0 = round(end_time - start_time,2)\n",
    "\n",
    "# Matriu de confusió\n",
    "matriu_conf_xgb_0 = np.round(np.mean(mc_xgb_0, axis = 0))\n",
    "matriu_conf_xgb_0_df = pd.DataFrame(matriu_conf_xgb_0, index=[\"Fatal\", \"Serious\", \"Slight\"],\n",
    "                                   columns=[\"Fatal\", \"Serious\", \"Slight\"])\n",
    "matriu_conf_xgb_0_df = np.round(matriu_conf_xgb_0_df)\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "np.set_printoptions(formatter={'float': '{:.0f}'.format})\n",
    "\n",
    "sns.heatmap(matriu_conf_xgb_0_df, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Classe predita', fontsize = 15)\n",
    "plt.ylabel('Classe vertadera', fontsize = 15)\n",
    "plt.xticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.yticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.title('Matriu de Confusió', fontsize = 17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca921f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mètriques\n",
    "np.set_printoptions(precision=2)\n",
    "print(especificitat_xgb_0_3)\n",
    "print(especificitat_xgb_0)\n",
    "print(mae_xgb_0)\n",
    "print(f1_xgb_0_3)\n",
    "print(precisio_xgb_0_3)\n",
    "print(balanced_accuracy_xgb_0)\n",
    "print(accuracy_xgb_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cec3fe",
   "metadata": {},
   "source": [
    "**Base de dades (classes balancejades - SMOTE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X = dades_equilibrades.drop('Accident_Severity', axis=1)\n",
    "y = dades_equilibrades['Accident_Severity']\n",
    "\n",
    "noms_classes = [\"Fatal\", \"Serious\", \"Slight\"] # 0, 1, 2\n",
    "\n",
    "model_xgb = XGBClassifier(\n",
    "    objective = 'multi:softmax',\n",
    "    n_estimators = 300,\n",
    "    max_depth = 15,\n",
    "    learning_rate = 0.05,\n",
    "    subsample = 0.5,\n",
    "    colsample_bytree = 0.8,\n",
    "    random_state = 123)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "accuracy_xgb = []\n",
    "precisio_xgb_3 = []\n",
    "precisio_xgb = []\n",
    "f1_xgb_3 = []\n",
    "f1_xgb = []\n",
    "sensibilitat_xgb_3 = []\n",
    "sensibilitat_xgb = []\n",
    "balanced_accuracy_xgb = []\n",
    "mc_xgb = []\n",
    "especificitat_xgb_3 = []\n",
    "especificitat_xgb = []\n",
    "mae_xgb = []\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model_xgb.fit(X_train, y_train)\n",
    "    y_pred_xgb = model_xgb.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "    cm_df = pd.DataFrame(cm, index = noms_classes, columns = noms_classes) # diferent cada vegada\n",
    "\n",
    "    # Mètriques\n",
    "    # Accuracy, Precisió, F1 Score, Sensibilitat\n",
    "    accuracy_xgb.append(accuracy_score(y_test, y_pred_xgb))\n",
    "    precisio_xgb_3.append(precision_score(y_test, y_pred_xgb, average=None))\n",
    "    precisio_xgb.append(precision_score(y_test, y_pred_xgb, average = 'weighted'))\n",
    "    f1_xgb_3.append(f1_score(y_test, y_pred_xgb, average = None))\n",
    "    f1_xgb.append(f1_score(y_test, y_pred_xgb, average = 'weighted'))\n",
    "    sensibilitat_xgb_3.append(recall_score(y_test, y_pred_xgb, average = None))\n",
    "    sensibilitat_ponderada = recall_score(y_test, y_pred_xgb, average = 'weighted')\n",
    "    sensibilitat_xgb.append(sensibilitat_ponderada)\n",
    "    mc_xgb.append(cm) # anem guardant totes les matrius de confusió\n",
    "   \n",
    "    # Especificitat\n",
    "    x = np.diag(cm)\n",
    "\n",
    "    for i in noms_classes:\n",
    "        index = noms_classes.index(i)\n",
    "        TN = cm_df.drop(i, axis = 0).drop(i, axis = 1).values.sum()\n",
    "        FP = cm_df[i].sum() - cm_df.loc[i, i]\n",
    "        especificitat = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        especificitat_xgb_3.append(especificitat)\n",
    "       \n",
    "    especificitat_ponderada = (especificitat_xgb_3[0]*x[0] + especificitat_xgb_3[1]*x[1] +\n",
    "                        especificitat_xgb_3[2]*x[2]) / sum(x)\n",
    "   \n",
    "    especificitat_xgb.append(especificitat_ponderada)\n",
    "\n",
    "\n",
    "    # Balanced Accuracy\n",
    "    ba = (sensibilitat_ponderada + especificitat_ponderada) / 2\n",
    "    balanced_accuracy_xgb.append(ba)\n",
    "   \n",
    "    # MAE\n",
    "    pesos = np.array([\n",
    "    [0, 1, 2],\n",
    "    [1, 0, 1],\n",
    "    [2, 1, 0]\n",
    "    ])\n",
    "\n",
    "    n = np.sum(cm)\n",
    "    mae = np.sum(cm * pesos) / n\n",
    "    mae_xgb.append(mae)\n",
    "    \n",
    "end_time = time.time()\n",
    "total_time_xgb = round(end_time - start_time,2)\n",
    "\n",
    "# Matriu de confusió\n",
    "matriu_conf_xgb = np.round(np.mean(mc_xgb, axis = 0))\n",
    "matriu_conf_xgb_df = pd.DataFrame(matriu_conf_xgb, index=[\"Fatal\", \"Serious\", \"Slight\"],\n",
    "                                   columns=[\"Fatal\", \"Serious\", \"Slight\"])\n",
    "matriu_conf_xgb_df = np.round(matriu_conf_xgb_df)\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "np.set_printoptions(formatter={'float': '{:.0f}'.format})\n",
    "\n",
    "sns.heatmap(matriu_conf_xgb_df, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Classe predita', fontsize = 15)\n",
    "plt.ylabel('Classe vertadera', fontsize = 15)\n",
    "plt.xticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.yticks(ticks = np.arange(len(noms_classes)), labels = noms_classes, rotation = 0, fontsize = 14)\n",
    "plt.title('Matriu de Confusió', fontsize = 17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a13389",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "print(especificitat_xgb_3)\n",
    "print(especificitat_xgb)\n",
    "print(mae_xgb)\n",
    "print(f1_xgb_3)\n",
    "print(precisio_xgb_3)\n",
    "print(balanced_accuracy_xgb)\n",
    "print(accuracy_xgb)\n",
    "print(f1_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a487eb",
   "metadata": {},
   "source": [
    "### 7. RESULTATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66842a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest \n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "metriques = pd.DataFrame({\n",
    "    'Model': ['Random Forest (Smote)', 'Random Forest (No Smote)'],\n",
    "    'Accuracy': [round(np.mean(accuracy_rf),3), round(np.mean(accuracy_rf_0),3)],\n",
    "    'Balanced Accuracy': [round(np.mean(balanced_accuracy_rf),3), round(np.mean(balanced_accuracy_rf_0),3)],\n",
    "    'Sensibilitat': [round(np.mean(sensibilitat_rf),3), round(np.mean(sensibilitat_rf_0),3)],\n",
    "    'Especificitat': [round(np.mean(especificitat_rf),3), round(np.mean(especificitat_rf_0),3)],\n",
    "    'Precisió': [round(np.mean(precisio_rf),3), round(np.mean(precisio_rf_0),3)],\n",
    "    'F1 Score': [round(np.mean(f1_rf),3), round(np.mean(f1_rf_0),3)],\n",
    "    'MAE': [round(np.mean(mae_rf),3), round(np.mean(mae_rf_0),3)],\n",
    "    'Temps transcorregut': [round(total_time_rf/60,3), round(total_time_rf_0/60,3)]\n",
    "})\n",
    "\n",
    "metriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe385a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# només regressió logistica multinomial\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "metriques = pd.DataFrame({\n",
    "    'Model': ['Regressió Logística Multinomial (Smote)', 'Regressió Logística Multinomial (No Smote)'],\n",
    "    'Accuracy': [round(np.mean(accuracy_reglog),3), round(np.mean(accuracy_reglog_0),3)],\n",
    "    'Balanced Accuracy': [round(np.mean(balanced_accuracy_reglog),3), round(np.mean(balanced_accuracy_reglog_0),3)],\n",
    "    'Sensibilitat': [round(np.mean(sensibilitat_reglog),3), round(np.mean(sensibilitat_reglog_0),3)],\n",
    "    'Especificitat': [round(np.mean(especificitat_reglog),3), round(np.mean(especificitat_reglog_0),3)],\n",
    "    'Precisió': [round(np.mean(precisio_reglog),3), round(np.mean(precisio_reglog_0),3)],\n",
    "    'F1 Score': [round(np.mean(f1_reglog),3), round(np.mean(f1_reglog_0),3)],\n",
    "    'MAE': [round(np.mean(mae_reglog),3), round(np.mean(mae_reglog_0),3)],\n",
    "    'Temps transcorregut': [round(total_time_reglog/60,3), round(total_time_reglog_0/60,3)]\n",
    "})\n",
    "\n",
    "metriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdcaf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# només regressió XGB\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "metriques = pd.DataFrame({\n",
    "    'Model': ['XGBoost (Smote)', 'XGBoost (No Smote)'],\n",
    "    'Accuracy': [round(np.mean(accuracy_xgb),3), round(np.mean(accuracy_xgb_0),3)],\n",
    "    'Balanced Accuracy': [round(np.mean(balanced_accuracy_xgb),3), round(np.mean(balanced_accuracy_xgb_0),3)],\n",
    "    'Sensibilitat': [round(np.mean(sensibilitat_xgb),3), round(np.mean(sensibilitat_xgb_0),3)],\n",
    "    'Especificitat': [round(np.mean(especificitat_xgb),3), round(np.mean(especificitat_xgb_0),3)],\n",
    "    'Precisió': [round(np.mean(precisio_xgb),3), round(np.mean(precisio_xgb_0),3)],\n",
    "    'F1 Score': [round(np.mean(f1_xgb),3), round(np.mean(f1_xgb_0),3)],\n",
    "    'MAE': [round(np.mean(mae_xgb),3), round(np.mean(mae_xgb_0),3)],\n",
    "    'Temps transcorregut': [round(total_time_xgb/60,3), round(total_time_xgb_0/60,3)]\n",
    "})\n",
    "\n",
    "metriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# junt\n",
    "\n",
    "metriques_smote = pd.DataFrame({\n",
    "    'Model': ['Regressió Logística Multinomial','Random Forest','XGBoost'],\n",
    "    'Accuracy': [round(np.mean(accuracy_reglog),3), round(np.mean(accuracy_rf),3), round(np.mean(accuracy_xgb),3)],\n",
    "    'Balanced Accuracy': [round(np.mean(balanced_accuracy_reglog),3), round(np.mean(balanced_accuracy_rf),3),\n",
    "                          round(np.mean(balanced_accuracy_xgb),3)],\n",
    "    'Sensibilitat': [round(np.mean(sensibilitat_reglog),3), round(np.mean(sensibilitat_rf),3),\n",
    "                     round(np.mean(sensibilitat_xgb),3)],\n",
    "    'Especificitat': [round(np.mean(especificitat_reglog),3), round(np.mean(especificitat_rf),3),\n",
    "                      round(np.mean(especificitat_xgb),3)],\n",
    "    'Precisió': [round(np.mean(precisio_reglog),3), round(np.mean(precisio_rf),3), round(np.mean(precisio_xgb),3)],\n",
    "    'F1 Score': [round(np.mean(f1_reglog),3), round(np.mean(f1_rf),3), round(np.mean(f1_xgb),3)],\n",
    "    'MAE': [round(np.mean(mae_reglog),3), round(np.mean(mae_rf),3), round(np.mean(mae_xgb),3)],\n",
    "    'Temps transcorregut': [round(total_time_reglog/60,3), round(total_time_rf/60,3), round(total_time_xgb/60,3), ]\n",
    "})\n",
    "\n",
    "\n",
    "metriques_no_smote = pd.DataFrame({\n",
    "    'Model': ['Regressió Logística Multinomial','Random Forest', 'XGBoost'],\n",
    "    'Accuracy': [round(np.mean(accuracy_reglog_0),3), round(np.mean(accuracy_rf_0),3), round(np.mean(accuracy_xgb_0),3)],\n",
    "    'Balanced Accuracy': [ round(np.mean(balanced_accuracy_reglog_0),3), round(np.mean(balanced_accuracy_rf_0),3),\n",
    "                          round(np.mean(balanced_accuracy_xgb_0),3)],\n",
    "    'Sensibilitat': [round(np.mean(sensibilitat_reglog_0),3), round(np.mean(sensibilitat_rf_0),3),\n",
    "                     round(np.mean(sensibilitat_xgb_0),3)],\n",
    "    'Especificitat': [round(np.mean(especificitat_reglog_0),3), round(np.mean(especificitat_rf_0),3),\n",
    "                      round(np.mean(especificitat_xgb_0),3)],\n",
    "    'Precisió': [round(np.mean(precisio_reglog_0),3), round(np.mean(precisio_rf_0),3), round(np.mean(precisio_xgb_0),3)],\n",
    "    'F1 Score': [round(np.mean(f1_reglog_0),3), round(np.mean(f1_rf_0),3), round(np.mean(f1_xgb_0),3)],\n",
    "    'MAE': [round(np.mean(mae_reglog_0),3), round(np.mean(mae_rf_0),3), round(np.mean(mae_xgb_0),3)],\n",
    "    'Temps transcorregut': [round(total_time_reglog_0/60,3), round(total_time_rf_0/60,3), round(total_time_xgb_0/60,3)]\n",
    "})\n",
    "\n",
    "#metriques_smote\n",
    "#metriques_no_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd9878",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    'Accuracy': '#006400',\n",
    "    'Balanced Accuracy': '#008000',\n",
    "    'Sensibilitat': '#228B22',\n",
    "    'Especificitat': '#3CB371',\n",
    "    'Precisió': '#32CD32',\n",
    "    'F1 Score': '#7CFC00',\n",
    "    'MAE': '#90EE90',\n",
    "    'Temps transcorregut': '#90EE90'\n",
    "}\n",
    "metriques_1 = metriques_no_smote.iloc[:, :-2]\n",
    "metriques_1 = metriques_1.melt(id_vars = 'Model', var_name = 'Mètrica', value_name = 'Valor')\n",
    "\n",
    "plt.figure(figsize = (9, 5))\n",
    "sns.barplot(data=metriques_1, x='Model', y='Valor', hue='Mètrica', palette=colors)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title('COMPARACIÓ DE MÈTRIQUES ENTRE ELS MODELS (No Smote)', fontsize = 16)\n",
    "plt.legend(title = 'Mètriques', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "metriques_2 = metriques_smote.iloc[:, :-2]\n",
    "metriques_2 = metriques_2.melt(id_vars = 'Model', var_name = 'Mètrica', value_name = 'Valor')\n",
    "plt.figure(figsize = (9, 5))\n",
    "sns.barplot(data=metriques_2, x='Model', y='Valor', hue='Mètrica', palette=colors)\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title('COMPARACIÓ DE MÈTRIQUES ENTRE ELS MODELS (Smote)', fontsize = 16)\n",
    "plt.legend(title = 'Mètriques', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors1 = {\n",
    "    'Regressió Logística Multinomial': '#006400',\n",
    "    'Random Forest': '#32CD32',\n",
    "    'XGBoost': '#7CFC00',\n",
    "}\n",
    "\n",
    "#dades inicials\n",
    "plt.figure(figsize = (15, 8))\n",
    "sns.barplot(data=metriques_1, x='Mètrica', y='Valor', hue='Model', palette=colors1)\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title('COMPARACIÓ DE MÈTRIQUES ENTRE ELS DIFERENTS\\nMODELS (No Smote)', fontsize = 20)\n",
    "plt.legend(title = 'Models', bbox_to_anchor=(1, 1), loc='upper left', fontsize = 16, title_fontsize = 16)\n",
    "plt.xticks(rotation = 45, fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#dades equilibrades\n",
    "plt.figure(figsize = (15, 8))\n",
    "sns.barplot(data=metriques_2, x='Mètrica', y='Valor', hue='Model', palette=colors1)\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title('COMPARACIÓ DE MÈTRIQUES ENTRE ELS DIFERENTS\\nMODELS PREDICTIUS', fontsize = 20)\n",
    "plt.legend(title = 'Models', bbox_to_anchor=(1, 1), loc='upper left', fontsize = 16, title_fontsize = 16)\n",
    "plt.xticks(rotation = 45, fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17547126",
   "metadata": {},
   "outputs": [],
   "source": [
    "metriques_2 = metriques_smote.iloc[:, :-1]\n",
    "metriques_2 = metriques_2.melt(id_vars = 'Model', var_name = 'Mètrica', value_name = 'Valor')\n",
    "colors2 = {\n",
    "    'Accuracy': '#006400',\n",
    "    'Balanced Accuracy': '#008000',\n",
    "    'Sensibilitat': '#228B22',\n",
    "    'Especificitat': '#3CB371',\n",
    "    'Precisió': '#32CD32',\n",
    "    'F1 Score': '#7CFC00',\n",
    "    'MAE': '#90EE90',\n",
    "    'Temps transcorregut': '#90EE90'\n",
    "}\n",
    "\n",
    "noms_metriques = ['Accuracy', 'Balanced Accuracy', 'Sensibilitat', 'Especificitat', 'Precisió', 'F1 Score', 'MAE']\n",
    "\n",
    "for metrica in noms_metriques:\n",
    "    plt.figure(figsize = (9, 5))\n",
    "    ax = sns.barplot(data=metriques_2.loc[metriques_2['Mètrica'] == metrica], x='Model', y='Valor', hue='Mètrica', palette=colors2)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        if p.get_height() > 0:\n",
    "            ax.annotate('{:.2f}'.format(p.get_height()), \n",
    "                        (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                        ha='center', va='bottom', fontsize=10, color='black', xytext=(0, 5), \n",
    "                        textcoords='offset points')\n",
    "    plt.gca().set_ylim([0, 1.03])\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(f'{metrica}') \n",
    "    plt.legend().remove()\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6bd502",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metriques_smote.iloc[:, [0, 7]]\n",
    "mae = mae.melt(id_vars = 'Model', var_name = 'MAE', value_name = 'Valor')\n",
    "\n",
    "plt.figure(figsize = (12, 5))\n",
    "sns.barplot(data=mae, x='MAE', y='Valor', hue='Model', palette=colors1)\n",
    "\n",
    "for p in plt.gca().patches:\n",
    "    if p.get_height() > 0:  \n",
    "        plt.gca().annotate('{}'.format(round(p.get_height(), 2)), \n",
    "                            (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                            ha='center', va='bottom', fontsize=14, color='black', xytext=(0, 5), \n",
    "                            textcoords='offset points')\n",
    "plt.gca().set_ylim([0, 40])\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title('COMPARACIÓ DEL MAE ENTRE ELS DIFERENTS\\nMODELS PREDICTIUS', fontsize = 17)\n",
    "plt.legend(title = 'Models', bbox_to_anchor=(1, 1), loc='upper left', fontsize = 14, title_fontsize = 14)\n",
    "plt.xticks(rotation = 0, fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.ylim(0,1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d8f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = metriques_smote.iloc[:, [0, -1]]\n",
    "temps = temps.melt(id_vars = 'Model', var_name = 'Eficiència', value_name = 'Valor')\n",
    "\n",
    "plt.figure(figsize = (12, 5))\n",
    "sns.barplot(data=temps, x='Eficiència', y='Valor', hue='Model', palette=colors1)\n",
    "\n",
    "for p in plt.gca().patches:\n",
    "    if p.get_height() > 0.1:  \n",
    "        plt.gca().annotate('{}'.format(round(p.get_height(), 2)), \n",
    "                            (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                            ha='center', va='bottom', fontsize=14, color='black', xytext=(0, 5), \n",
    "                            textcoords='offset points')\n",
    "plt.gca().set_ylim([0, 40])\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title('COMPARACIÓ DE L\\'EFICIÈNCIA ENTRE ELS DIFERENTS\\nMODELS PREDICTIUS', fontsize = 17)\n",
    "plt.legend(title = 'Models', bbox_to_anchor=(1, 1), loc='upper left', fontsize = 14, title_fontsize = 14)\n",
    "plt.xticks(rotation = 0, fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.ylim(0,80)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4351980",
   "metadata": {},
   "source": [
    "### 8.CONCLUSIONS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
